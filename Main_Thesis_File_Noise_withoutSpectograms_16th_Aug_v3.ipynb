{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiteshJindal/Thesis_Audio_Uncertainity/blob/main/Main_Thesis_File_Noise_withoutSpectograms_16th_Aug_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvk7U_XlWMTw"
      },
      "outputs": [],
      "source": [
        "!pip install pydub\n",
        "!pip install transformers\n",
        "!pip install keras-tuner\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import kerastuner as kt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization, GlobalMaxPooling1D, SpatialDropout1D, Flatten, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from google.colab import drive\n",
        "from collections import Counter\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner import HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "MSTlzXnfWz8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Create an empty DataFrame to store the results\n",
        "final_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'Phones'])\n",
        "\n",
        "directory = '/gdrive/MyDrive/Code_Hitesh_Thesis/textGrid_training_0_5/'\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "    start_time, end_time, label = None, None, None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('xmin'):\n",
        "            start_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('xmax'):\n",
        "            end_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('text'):\n",
        "            label = line.split('=')[1].strip().strip('\"')\n",
        "            if start_time is not None and end_time is not None and label is not None:\n",
        "                data.append((start_time, end_time, label))\n",
        "                start_time, end_time, label = None, None, None\n",
        "\n",
        "    return data\n",
        "\n",
        "def textgrid_to_dataframe(file_path):\n",
        "    data = parse_textgrid(file_path)\n",
        "    df = pd.DataFrame(data, columns=['Start Time', 'End Time', 'Label'])\n",
        "    return df\n",
        "\n",
        "# Create an empty DataFrame to store the results\n",
        "final_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'Phones'])\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.TextGrid'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Process the file and obtain the necessary dataframes\n",
        "        df = textgrid_to_dataframe(file_path)\n",
        "\n",
        "        # Get the indices of the matched rows\n",
        "        indices = df.index[(df['Start Time'] == df['Start Time'].iloc[0]) & (df['End Time'] == df['End Time'].iloc[0])]\n",
        "\n",
        "        # Split the DataFrame based on indices\n",
        "        first_df = df.loc[:indices[-1]]\n",
        "        second_df = df.loc[indices[-1]+1:]\n",
        "\n",
        "        # Remove rows with blank or null labels from first_df\n",
        "        first_df = first_df[first_df['Label'].notnull() & (first_df['Label'] != \"\")]\n",
        "\n",
        "        # Remove rows with blank or null labels from second_df\n",
        "        second_df = second_df[second_df['Label'].notnull() & (second_df['Label'] != \"\")]\n",
        "\n",
        "        # Combine labels from first_df into a single sentence\n",
        "        combined_sentence = ' '.join(first_df['Label'].tolist())\n",
        "\n",
        "        # Create Combined_df with the combined sentence\n",
        "        combined_df = pd.DataFrame({'Transcript': [combined_sentence]})\n",
        "\n",
        "        # Find the highest occurring string in second_df\n",
        "        phoneme_likelihood = second_df['Label'].mode().iloc[0]\n",
        "\n",
        "        # Create Transcript DataFrame with the highest occurring string\n",
        "        transcript_df = pd.DataFrame({'phoneme_likelihood': [phoneme_likelihood]})\n",
        "\n",
        "        # Combine labels from second_df into a list\n",
        "        phones_list = second_df['Label'].tolist()\n",
        "\n",
        "        # Create Phones DataFrame with the list of phones\n",
        "        phones_df = pd.DataFrame({'Phones': [phones_list]})\n",
        "\n",
        "        # Concatenate the DataFrames and append to the final_df\n",
        "        result_df = pd.concat([combined_df, transcript_df, phones_df], axis=1)\n",
        "        final_df = pd.concat([final_df, result_df], ignore_index=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "g0Ot1MxY0_cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Create an empty DataFrame to store the results\n",
        "final_test_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'Phones'])\n",
        "\n",
        "directory = '/gdrive/MyDrive/Code_Hitesh_Thesis/textGrid_test_0_5/'\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "    start_time, end_time, label = None, None, None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('xmin'):\n",
        "            start_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('xmax'):\n",
        "            end_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('text'):\n",
        "            label = line.split('=')[1].strip().strip('\"')\n",
        "            if start_time is not None and end_time is not None and label is not None:\n",
        "                data.append((start_time, end_time, label))\n",
        "                start_time, end_time, label = None, None, None\n",
        "\n",
        "    return data\n",
        "\n",
        "def textgrid_to_dataframe(file_path):\n",
        "    data = parse_textgrid(file_path)\n",
        "    df = pd.DataFrame(data, columns=['Start Time', 'End Time', 'Label'])\n",
        "    return df\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.TextGrid'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Process the file and obtain the necessary dataframes\n",
        "        df = textgrid_to_dataframe(file_path)\n",
        "\n",
        "        # Get the indices of the matched rows\n",
        "        indices = df.index[(df['Start Time'] == df['Start Time'].iloc[0]) & (df['End Time'] == df['End Time'].iloc[0])]\n",
        "\n",
        "        # Split the DataFrame based on indices\n",
        "        first_test_df = df.loc[:indices[-1]]\n",
        "        second_test_df = df.loc[indices[-1]+1:]\n",
        "\n",
        "        # Remove rows with blank or null labels from first_df\n",
        "        first_test_df = first_test_df[first_test_df['Label'].notnull() & (first_test_df['Label'] != \"\")]\n",
        "\n",
        "        # Remove rows with blank or null labels from second_df\n",
        "        second_test_df = second_test_df[second_test_df['Label'].notnull() & (second_test_df['Label'] != \"\")]\n",
        "\n",
        "        # Combine labels from first_df into a single sentence\n",
        "        combined_sentence_test = ' '.join(first_test_df['Label'].tolist())\n",
        "\n",
        "        # Create Combined_df with the combined sentence\n",
        "        combined_test_df = pd.DataFrame({'Transcript': [combined_sentence_test]})\n",
        "\n",
        "        # Find the highest occurring string in second_df\n",
        "        phoneme_likelihood_test = second_test_df['Label'].mode().iloc[0]\n",
        "\n",
        "        # Create Transcript DataFrame with the highest occurring string\n",
        "        transcript_test_df = pd.DataFrame({'phoneme_likelihood': [phoneme_likelihood_test]})\n",
        "\n",
        "        # Create Phones DataFrame with the list of phones\n",
        "        phones_test_df = pd.DataFrame({'Phones': [second_test_df['Label'].tolist()]})\n",
        "\n",
        "        # Concatenate the DataFrames and append to the final_df\n",
        "        result_test_df = pd.concat([combined_test_df, transcript_test_df, phones_test_df], axis=1)\n",
        "        final_test_df = pd.concat([final_test_df, result_test_df], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "XjgXJ2B4Iuns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary dataframe before resampling the data to balance the imbalanced dataset\n",
        "X_temp = final_df\n",
        "X_temp = X_temp.drop('phoneme_likelihood', axis=1)\n"
      ],
      "metadata": {
        "id": "qdoJr2BIcrtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the desired minority class count based on 0.5 times the majority class count\n",
        "y = final_df['phoneme_likelihood']\n",
        "majority_class_count = max(Counter(y).values())\n",
        "desired_minority_class_count = int(majority_class_count * 0.1)\n",
        "\n",
        "# Initialize RandomOverSampler with a custom sampling strategy\n",
        "sampling_strategy = {label: desired_minority_class_count for label, count in Counter(y).items() if count < desired_minority_class_count}\n",
        "oversampler = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=0)\n",
        "\n",
        "# Perform oversampling on the training data\n",
        "X_temp_resampled, y_resampled = oversampler.fit_resample(X_temp, y)\n",
        "\n",
        "# Print class distribution after oversampling\n",
        "print(\"Class distribution after oversampling:\", sorted(Counter(y_resampled).items()))"
      ],
      "metadata": {
        "id": "0_2TBeKDlIwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Without Keras Tuner and without cross validation\n",
        "\n",
        "# Extract the input features and response variable\n",
        "X = X_temp_resampled['Transcript']\n",
        "y = y_resampled\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the training data\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_test_seq))\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Encode the response variable\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y)  # Fit the label encoder on all labels in y\n",
        "y_encoded = label_encoder.transform(y)\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "\n",
        "# oversampler = RandomOverSampler(random_state = 0 , sampling_strategy= \"minority\")\n",
        "# X_train_resampled, y_train_resampled = ADASYN(n_neighbors=3).fit_resample(X_train_padded, y_train_encoded)\n",
        "\n",
        "\n",
        "# Define the improved model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=200, input_length=max_length))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(units=64, activation='tanh', kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)  # You can adjust the patience value\n",
        "\n",
        "# Define the model checkpoint callback to save the best model during training\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_padded, y_train_encoded, epochs=100, batch_size=128, validation_data=(X_test_padded, y_test_encoded), callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "\n",
        "# Calculate training and validation accuracy\n",
        "train_accuracy = model.evaluate(X_train_padded, y_train_encoded)[1]\n",
        "val_accuracy = model.evaluate(X_test_padded, y_test_encoded)[1]\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ],
      "metadata": {
        "id": "LVgirleu7tJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WIth K fold cross validation\n",
        "\n",
        "# Load and preprocess your dataset\n",
        "# Assuming you have already loaded the dataset into X_temp_resampled and y_resampled\n",
        "\n",
        "# Initialize k-fold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store training and validation accuracies\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for train_index, val_index in kfold.split(X_temp_resampled):\n",
        "    X_train, X_val = X_temp_resampled.iloc[train_index], X_temp_resampled.iloc[val_index]\n",
        "    y_train, y_val = y_resampled[train_index], y_resampled[val_index]\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "\n",
        "    # Fit the tokenizer on the training data\n",
        "    tokenizer.fit_on_texts(X_train['Transcript'])\n",
        "\n",
        "    # Convert text to sequences\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train['Transcript'])\n",
        "    X_val_seq = tokenizer.texts_to_sequences(X_val['Transcript'])\n",
        "\n",
        "    # Pad sequences to have the same length\n",
        "    max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_val_seq))\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "    X_val_padded = pad_sequences(X_val_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Encode the response variable\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(y_train)  # Fit the label encoder on train labels\n",
        "    y_train_encoded = label_encoder.transform(y_train)\n",
        "    y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "    # Define the improved model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=200, input_length=max_length))\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=64, activation='tanh', kernel_regularizer=l2(0.001)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    # Define EarlyStopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Define the model checkpoint callback\n",
        "    checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "    # Train the model with k-fold cross-validation\n",
        "    history = model.fit(X_train_padded, y_train_encoded, epochs=100, batch_size=128,\n",
        "                        validation_data=(X_val_padded, y_val_encoded),\n",
        "                        callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "    # Calculate training and validation accuracy\n",
        "    train_accuracy = model.evaluate(X_train_padded, y_train_encoded)[1]\n",
        "    val_accuracy = model.evaluate(X_val_padded, y_val_encoded)[1]\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "# Print average training and validation accuracies across folds\n",
        "print(\"Average Training Accuracy:\", np.mean(train_accuracies))\n",
        "print(\"Average Validation Accuracy:\", np.mean(val_accuracies))"
      ],
      "metadata": {
        "id": "-LsYQnnI9_sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With both k-fold cross validation and Keras Tuner\n",
        "\n",
        "# Load and preprocess your dataset\n",
        "# Assuming you have already loaded the dataset into X_temp_resampled and y_resampled\n",
        "\n",
        "# Initialize k-fold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store validation accuracies from each fold\n",
        "val_accuracies = []\n",
        "\n",
        "for fold_index, (train_index, val_index) in enumerate(kfold.split(X_temp_resampled)):\n",
        "    X_train, X_val = X_temp_resampled.iloc[train_index], X_temp_resampled.iloc[val_index]\n",
        "    y_train, y_val = y_resampled[train_index], y_resampled[val_index]\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "\n",
        "    # Fit the tokenizer on the training data\n",
        "    tokenizer.fit_on_texts(X_train['Transcript'])\n",
        "\n",
        "    # Convert text to sequences\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train['Transcript'])\n",
        "    X_val_seq = tokenizer.texts_to_sequences(X_val['Transcript'])\n",
        "\n",
        "    # Pad sequences to have the same length\n",
        "    max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_val_seq))\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "    X_val_padded = pad_sequences(X_val_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Encode the response variable\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(y_train)  # Fit the label encoder on train labels\n",
        "    y_train_encoded = label_encoder.transform(y_train)\n",
        "    y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "    # Define the improved model using Keras Tuner\n",
        "    def build_model(hp):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=hp.Int('embedding_dim', min_value=32, max_value=256, step=32), input_length=max_length))\n",
        "        model.add(Conv1D(filters=hp.Int('filters', min_value=32, max_value=256, step=32), kernel_size=5, activation='relu'))\n",
        "        model.add(GlobalMaxPooling1D())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)))\n",
        "        # Add another Conv1D layer\n",
        "        model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "        model.add(Dropout(hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)))\n",
        "        model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # Initialize the tuner\n",
        "    tuner = RandomSearch(\n",
        "        build_model,\n",
        "        objective='val_accuracy',\n",
        "        max_trials=10,\n",
        "        directory=f'tuner_results_fold_{fold_index + 1}',  # Path to store the results for each fold\n",
        "        project_name='phoneme_tuning')  # A unique project name\n",
        "\n",
        "    # Perform hyperparameter search\n",
        "    tuner.search(X_train_padded, y_train_encoded, epochs=20, batch_size=128, validation_data=(X_val_padded, y_val_encoded))\n",
        "\n",
        "    # Get the best model\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    # Calculate validation accuracy for the best model\n",
        "    val_accuracy = best_model.evaluate(X_val_padded, y_val_encoded)[1]\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "# Print average validation accuracy across folds\n",
        "print(\"Average Validation Accuracy:\", np.mean(val_accuracies))\n"
      ],
      "metadata": {
        "id": "ReFslM15BJvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have a new DataFrame called 'new_data' with the same column names as 'final_df'\n",
        "#X_temp_resampled_test, y_resampled_test\n",
        "\n",
        "# Extract the input features from the new data\n",
        "X_new = final_test_df['Transcript']\n",
        "\n",
        "# Convert text to sequences using the tokenizer fitted on the training data\n",
        "X_new_seq = tokenizer.texts_to_sequences(X_new)\n",
        "\n",
        "# Pad sequences to have the same length as the training data\n",
        "X_new_padded = pad_sequences(X_new_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Make predictions on the new data\n",
        "predictions = model.predict(X_new_padded)\n",
        "\n",
        "# Decode the predicted labels\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "print(predicted_labels)\n",
        "# Calculate the accuracy on the new data\n",
        "accuracy = np.mean(predicted_labels == final_test_df['phoneme_likelihood'])\n",
        "\n",
        "print(\"Accuracy on New Data:\", accuracy)\n"
      ],
      "metadata": {
        "id": "VJcIq4mp8QS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "true_labels = final_test_df['phoneme_likelihood']\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n"
      ],
      "metadata": {
        "id": "dcJmnowCScyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the NumPy array to a pandas Series\n",
        "predicted_labels_series = pd.Series(predicted_labels)\n",
        "\n",
        "# Now you can use value_counts()\n",
        "predicted_labels_counts = predicted_labels_series.value_counts()\n",
        "\n",
        "print(predicted_labels_counts)"
      ],
      "metadata": {
        "id": "l9O71nTs-Pim"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}