{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x52V2CWwzJTI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9678b7e8-2ef9-44fc-f7d1-ce81175497f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.4-py3-none-any.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (3.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.12.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.53.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.12.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.2.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.32.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (16.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.12.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.7)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (23.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (67.6.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->keras-tuner) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=2.0->keras-tuner) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=2.0->keras-tuner) (0.0.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (2.17.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (3.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (6.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (3.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.4 kt-legacy-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeech\n",
        "import deepspeech\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HAx1aA1H3cX",
        "outputId": "c44daa51-c1e4-4608-f067-e274e114b3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepspeech\n",
            "  Downloading deepspeech-0.9.3-cp39-cp39-manylinux1_x86_64.whl (9.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.9/dist-packages (from deepspeech) (1.22.4)\n",
            "Installing collected packages: deepspeech\n",
            "Successfully installed deepspeech-0.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load packages\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from keras.utils import np_utils\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from google.cloud import speech_v1\n",
        "#from google.cloud.speech_v1 import enums"
      ],
      "metadata": {
        "id": "sKW695G_0EHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a46412-8bda-4dcd-b600-38f97bb91cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-737ec964e959>:18: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import RandomSearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and initalize parameters\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# Define the directory where the audio files are stored\n",
        "data_dir = '/gdrive/MyDrive/VIVAE/full_set/'\n",
        "model_dir = '/gdrive/MyDrive/VIVAE/deepspeech-0.9.3-models.pbmm' \n",
        "model_score_dir = '/gdrive/MyDrive/VIVAE/deepspeech-0.9.3-models.scorer' \n",
        "\n",
        "# Define the number of classes in the dataset\n",
        "num_classes = 6\n",
        "\n",
        "LOG_DIR = f\"{int(time.time())}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-270a7yg0Fea",
        "outputId": "e6eb069f-64ea-4a04-c748-1b9ea88f8203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phoneme_dict = {'AA': 0, 'AE': 1, 'AH': 2, 'AO': 3, 'AW': 4, 'AY': 5, 'B': 6, 'CH': 7, 'D': 8, 'DH': 9, 'EH': 10, 'ER': 11, 'EY': 12, 'F': 13, 'G': 14, 'HH': 15, 'IH': 16, 'IY': 17, 'JH': 18, 'K': 19, 'L': 20, 'M': 21, 'N': 22, 'NG': 23, 'OW': 24, 'OY': 25, 'P': 26, 'R': 27, 'S': 28, 'SH': 29, 'T': 30, 'TH': 31, 'UH': 32, 'UW': 33, 'V': 34, 'W': 35, 'Y': 36, 'Z': 37, 'ZH': 38}"
      ],
      "metadata": {
        "id": "At1imU11FqXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioClassifier:\n",
        "    def __init__(self, data_path, LOG_DIR):\n",
        "        self.data_path = data_path\n",
        "        self.LOG_DIR = LOG_DIR\n",
        "\n",
        "    def load_data(self):\n",
        "        mfccs = []\n",
        "        labels = []\n",
        "        for filename in os.listdir(self.data_path):\n",
        "            # Load the audio file\n",
        "            audio_path = os.path.join(self.data_path, filename)\n",
        "            audio, sr1 = librosa.load(audio_path)\n",
        "            phonemes = []\n",
        "\n",
        "            # Extract the label from the filename\n",
        "            label = filename.split(\"_\")[1]\n",
        "            noise = np.random.randn(len(audio))\n",
        "            norm_audio = audio / np.max(np.abs(audio))\n",
        "            noise = noise / np.max(np.abs(noise))\n",
        "            noisy_audio = norm_audio + 10**(-10/20) * noise\n",
        "\n",
        "            # Convert the audio to mfccs\n",
        "            mfcc = librosa.feature.mfcc(y=noisy_audio, sr=sr1, n_mfcc=20)\n",
        "\n",
        "\n",
        "            # Pad the MFCC array to ensure that it has a consistent shape\n",
        "            pad_width = 20 - mfcc.shape[1]\n",
        "            if pad_width > 0:\n",
        "                mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "            elif pad_width < 0:\n",
        "                mfcc = mfcc[:, :20]\n",
        "           \n",
        "            # Transcribe the audio to phonemes\n",
        "            ds = deepspeech.Model(model_dir)\n",
        "            ds.enableExternalScorer(model_score_dir)\n",
        "            audio_data, _ = librosa.load(audio_path, sr=ds.sampleRate())\n",
        "            audio_data_int16 = (audio_data * np.iinfo(np.int16).max).astype(np.int16)\n",
        "            phoneme_seq = ds.stt(audio_data_int16)\n",
        "            phonemes.append(phoneme_seq)\n",
        "            mfccs.append(mfcc)\n",
        "            labels.append(label)\n",
        "\n",
        "        # Convert the data to numpy arrays\n",
        "        X = np.array(mfccs)\n",
        "        y = np.array(labels)\n",
        "        X_new = np.array(phonemes)\n",
        "\n",
        "        # Convert the labels to categorical\n",
        "        self.label_map = {label: i for i, label in enumerate(set(labels))}\n",
        "        y = np.array([self.label_map[label] for label in labels])\n",
        "        y = to_categorical(y)\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_new, y, test_size=0.2)\n",
        "\n",
        "    def build_model(self, hp):\n",
        "        model = keras.Sequential()\n",
        "        model.add(layers.Flatten(input_shape=(self.X_train.shape[1], self.X_train.shape[2])))\n",
        "\n",
        "        # Tune the number of layers and units in each layer\n",
        "        for i in range(hp.Int('num_layers', 1, 4)):\n",
        "            model.add(layers.Dense(units=hp.Int(f'conv_{i}_units', min_value=32, max_value=512, step=32), activation='relu'))\n",
        "            model.add(layers.Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "\n",
        "        model.add(layers.Dense(self.y_train.shape[1], activation='softmax'))\n",
        "\n",
        "\n",
        "        # Tune the learning rate for the optimizer\n",
        "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics=['accuracy'], experimental_run_tf_function=False)\n",
        "        return model\n",
        "\n",
        "    # Define a function to sample predictions from the model with dropout\n",
        "    def predict_with_dropout(self, X, n_samples):\n",
        "        result = np.zeros((n_samples, X.shape[0], 6))\n",
        "        for i in range(n_samples):\n",
        "           result[i] = self.model.predict(X)\n",
        "        return result\n",
        "\n",
        "    def plot_history(self, history):\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        axs[0].plot(history.history['accuracy'], label='train')\n",
        "        axs[0].plot(history.history['val_accuracy'], label='test')\n",
        "        axs[0].set_title('Model Accuracy')\n",
        "        axs[0].set_xlabel('Epoch')\n",
        "        axs[0].set_ylabel('Accuracy')\n",
        "        axs[0].legend()\n",
        "\n",
        "        axs[1].plot(history.history['loss'], label='train')\n",
        "        axs[1].plot(history.history['val_loss'], label='test')\n",
        "        axs[1].set_title('Model Loss')\n",
        "        axs[1].set_xlabel('Epoch')\n",
        "        axs[1].set_ylabel('Loss')\n",
        "        axs[1].legend()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def predict(self, new_data_path):\n",
        "        # Load and preprocess the new data\n",
        "        mfccs = []\n",
        "        for filename in os.listdir(new_data_path):\n",
        "            audio_path = os.path.join(new_data_path, filename)\n",
        "            audio, sr = librosa.load(audio_path)\n",
        "            mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)\n",
        "            pad_width = 20 - mfcc.shape[1]\n",
        "            if pad_width > 0:\n",
        "                mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "            elif pad_width < 0:\n",
        "                mfcc = mfcc[:, :20]\n",
        "            mfccs.append(mfcc)\n",
        "        X_new = np.array(mfccs)\n",
        "\n",
        "        # Convert the labels to categorical\n",
        "        label_map = self.label_map\n",
        "        y_new = np.array([label_map[filename.split(\"_\")[1]] for filename in os.listdir(new_data_path)])\n",
        "        y_new = to_categorical(y_new)\n",
        "\n",
        "        # Make predictions on the new data\n",
        "        preds = self.model.predict(X_new)\n",
        "        conf_scores = np.max(preds, axis=1) # Assumes a classification model with predict_proba method\n",
        "        # Convert predicted probabilities to class labels\n",
        "        y_pred_labels = np.argmax(preds, axis=1)\n",
        "        y_true_labels = np.argmax(y_new, axis=1)\n",
        "\n",
        "        conf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "\n",
        "        # Calculate per-class uncertainty\n",
        "        class_uncertainty = []\n",
        "        for i in range(len(conf_matrix)):\n",
        "           correct = conf_matrix[i, i]\n",
        "           incorrect = np.sum(conf_matrix[i]) - correct\n",
        "           total = correct + incorrect\n",
        "        if total == 0:\n",
        "           class_uncertainty.append(0)\n",
        "        else:\n",
        "           class_uncertainty.append(1 - (correct/total))\n",
        "\n",
        "        # Calculate average uncertainty\n",
        "        avg_uncertainty = 1 - np.mean(conf_scores)\n",
        "\n",
        "        # Print results\n",
        "        print(\"Per-class uncertainty:\", class_uncertainty)\n",
        "        print(\"Average uncertainty:\", avg_uncertainty)\n",
        "\n",
        "        # Get the predicted class from the probabilities\n",
        "        preds_classes = np.argmax(preds, axis=1)\n",
        "\n",
        "        # Get the actual class from the one-hot encoded array\n",
        "        actual_classes = np.argmax(y_new, axis=1)\n",
        "        score = self.model.evaluate(X_new,y_new)\n",
        "        # Print the predicted and actual classes\n",
        "\n",
        "        print('Test loss:', score[0])\n",
        "        print('New data accuracy:', score[1])\n",
        "\n",
        "        print(\"Predictions:\", preds_classes)\n",
        "        print(\"Actual:\", actual_classes)\n",
        "\n",
        "        \n",
        "    def search_hyperparameters1(self, X_train, y_train, X_test, y_test, max_trials=10, executions_per_trial=5, LOG_DIR='audio_classifier'):\n",
        "        # Define the tuner\n",
        "        tuner = RandomSearch(self.build_model, objective='val_accuracy', max_trials=max_trials, \n",
        "                             executions_per_trial=executions_per_trial, directory=LOG_DIR, project_name='audio_classifier')\n",
        "\n",
        "        # Search for the best hyperparameter configuration\n",
        "        # Define the KFold cross-validator\n",
        "        kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "        # Perform cross-validation and search for the best hyperparameter configuration\n",
        "        for train_index, val_index in kfold.split(X_train):\n",
        "           X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "           y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "           tuner.search(X_train_fold, y_train_fold, epochs=10, validation_data=(X_val_fold, y_val_fold))\n",
        "\n",
        "        print(tuner.get_best_hyperparameters()[0].values)\n",
        "        print(tuner.get_best_models()[0].summary())\n",
        "\n",
        "        # Get the best hyperparameters\n",
        "        best_hp = tuner.get_best_hyperparameters(num_trials=3)[0]\n",
        "\n",
        "        # Build the model with the best hyperparameters\n",
        "        self.model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "        # Define EarlyStopping callback\n",
        "        earlystop = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.01, verbose=1, mode='auto')\n",
        "\n",
        "        # Fit the model to the training data\n",
        "        history = self.model.fit(X_train, y_train, batch_size=32, epochs=150 , validation_data=(X_test, y_test))\n",
        "\n",
        "        # Plot the training history\n",
        "        self.plot_history(history)\n",
        "\n",
        "        # Predict the probabilities for each class \n",
        "        preds = self.model.predict(X_test)\n",
        "\n",
        "        # Get the predicted class from the probabilities\n",
        "        preds_classes = np.argmax(preds, axis=1)\n",
        "\n",
        "        # Get the actual class from the one-hot encoded array\n",
        "        actual_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        score = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "        print('Test loss:', score[0])\n",
        "        print('New data accuracy:', score[1])\n",
        "        \n",
        "        # Print the predicted and actual classes for the first 10 samples in the test set\n",
        "        print(\"Predictions:\", preds_classes[:20])\n",
        "        print(\"Actual:\", actual_classes[:20])\n",
        "\n",
        "    def search_hyperparameters(self, X_train, y_train, X_test, y_test, max_trials=10, executions_per_trial=5, LOG_DIR='audio_classifier', num_folds=5):\n",
        "    # Define the tuner\n",
        "        tuner = RandomSearch(self.build_model, objective='val_accuracy', max_trials=max_trials, \n",
        "                         executions_per_trial=executions_per_trial, directory=LOG_DIR, project_name='audio_classifier')\n",
        "\n",
        "        # Search for the best hyperparameter configuration\n",
        "        tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test), \n",
        "                    validation_split=1/num_folds)\n",
        "\n",
        "        # Get the best hyperparameters\n",
        "        best_hp = tuner.get_best_hyperparameters(num_trials=3)[0]\n",
        "\n",
        "        # Build the model with the best hyperparameters\n",
        "        self.model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "        # Define EarlyStopping callback\n",
        "        earlystop = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.01, verbose=1, mode='auto')\n",
        "\n",
        "        # Define KFold cross-validation\n",
        "        kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "       # Train and evaluate the model using KFold cross-validation\n",
        "        scores = []\n",
        "\n",
        "        for train_index, val_index in kfold.split(X_train):\n",
        "           train_data = X_train[train_index]\n",
        "           train_labels = y_train[train_index]\n",
        "           val_data = X_train[val_index]\n",
        "           val_labels = y_train[val_index]\n",
        "\n",
        "           # Fit the model to the training data for this fold\n",
        "           history = self.model.fit(train_data, train_labels, batch_size=32, epochs=100 , validation_data=(val_data, val_labels))\n",
        "\n",
        "           # Evaluate the model on the validation data for this fold\n",
        "           score = self.model.evaluate(val_data, val_labels, verbose=0)\n",
        "           scores.append(score[1])\n",
        "\n",
        "        # Print the average accuracy over all folds\n",
        "        print('Cross-validation accuracy:', np.mean(scores))\n",
        "\n",
        "        # Fit the final model to all the training data\n",
        "        self.model.fit(X_train, y_train, batch_size=32, epochs=100 , validation_data=(X_test, y_test))\n",
        "\n",
        "        # Predict the probabilities for each class \n",
        "        preds = self.model.predict(X_test)\n",
        "\n",
        "        # Get the predicted class from the probabilities\n",
        "        preds_classes = np.argmax(preds, axis=1)\n",
        "\n",
        "        # Get the actual class from the one-hot encoded array\n",
        "        actual_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        score = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "        print('Test loss:', score[0])\n",
        "        print('New data accuracy:', score[1])\n",
        "\n",
        "        # Print the predicted and actual classes for the first 20 samples in the test set\n",
        "        print(\"Predictions:\", preds_classes[:20])\n",
        "        print(\"Actual:\", actual_classes[:20])\n"
      ],
      "metadata": {
        "id": "bo7B8Pqo0acF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the AudioClassifier\n",
        "classifier = AudioClassifier(data_path= data_dir, LOG_DIR= LOG_DIR)\n",
        "\n",
        "# Load the data\n",
        "classifier.load_data()\n",
        "\n",
        "classifier.search_hyperparameters(classifier.X_train, classifier.y_train, classifier.X_test, classifier.y_test)\n",
        "\n",
        "predictions = classifier.predict_with_dropout(classifier.X_test, 100)\n",
        "\n",
        "mean_prediction = np.mean(predictions, axis=0)\n",
        "std_prediction = np.std(predictions, axis=0)\n",
        "\n",
        "# Calculate the entropy of the predictions\n",
        "entropy = -np.sum(mean_prediction * np.log(mean_prediction), axis=1)\n",
        "uncertainty = np.mean(entropy)\n",
        "\n",
        "print('Uncertainty:', uncertainty)\n",
        "\n",
        "# Search for the best hyperparameter configuration"
      ],
      "metadata": {
        "id": "_F_wBily0by-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "3a8f549c-ea5a-4b37-f042-b37a86729fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5330b9c32455>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a916f69e4934>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Split the data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 1105]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the new audio files\n",
        "new_data_path = '/gdrive/MyDrive/VIVAE/core_set/'\n",
        "\n",
        "# Get the predicted class probabilities for the new data\n",
        "preds = classifier.predict(new_data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elHV634V5ZuE",
        "outputId": "01bc0323-785d-4b21-9ee9-5273c4ff449d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 5ms/step\n",
            "Per-class uncertainty: [0.9135802469135803]\n",
            "Average uncertainty: 0.05178570747375488\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 17.8631 - accuracy: 0.3429\n",
            "Test loss: 17.863059997558594\n",
            "New data accuracy: 0.34285715222358704\n",
            "Predictions: [1 4 2 4 3 0 4 3 2 1 3 2 2 5 4 0 0 4 2 3 4 2 0 1 5 1 4 0 1 5 2 4 4 0 4 4 0\n",
            " 0 3 4 0 4 4 4 2 3 4 3 4 3 0 4 3 4 4 4 4 3 3 4 0 4 0 4 0 0 2 0 4 3 3 4 4 0\n",
            " 4 2 4 4 0 4 3 4 4 2 2 0 3 3 3 4 4 3 4 4 4 4 0 0 3 0 4 4 4 0 4 3 0 4 4 4 3\n",
            " 0 5 4 4 3 0 4 3 4 4 4 2 1 4 4 3 4 4 2 2 0 0 0 1 3 4 4 4 4 4 1 3 3 2 4 4 4\n",
            " 4 3 4 2 4 2 0 0 4 3 0 0 0 4 4 4 5 4 0 4 3 4 4 0 4 0 4 4 4 3 0 4 4 4 2 4 4\n",
            " 3 0 4 4 3 4 0 4 4 1 0 4 3 2 4 1 4 2 0 3 0 3 4 4 5 4 1 0 4 4 3 4 4 3 3 4 4\n",
            " 0 3 4 4 4 2 3 0 4 4 4 0 4 4 4 3 4 4 4 3 0 0 4 3 4 1 3 2 4 2 0 3 4 2 1 4 4\n",
            " 0 4 5 0 3 0 0 2 0 4 4 3 4 0 5 4 0 3 4 4 4 3 0 2 0 2 4 3 4 4 4 2 2 0 2 4 2\n",
            " 3 0 0 0 0 4 0 4 3 3 4 3 3 3 3 4 4 0 3 2 4 1 4 5 3 0 4 4 2 4 4 4 4 0 2 3 3\n",
            " 2 4 4 4 3 4 3 4 4 3 2 1 4 4 0 4 3 0 4 3 0 3 0 4 0 4 5 0 5 4 2 4 4 4 3 0 4\n",
            " 4 4 2 3 4 1 0 4 3 4 4 0 4 4 4 4 4 0 4 0 4 0 3 0 3 4 4 2 4 3 4 4 3 3 4 4 0\n",
            " 4 4 3 0 4 4 3 0 4 0 3 3 3 4 4 3 4 4 4 4 4 0 4 3 4 4 0 4 0 4 3 4 0 3 4 0 4\n",
            " 3 4 4 4 2 2 3 4 3 4 3 3 0 3 4 2 0 1 3 3 4 5 2 4 2 4 4 0 2 0 0 5 3 0 4 4 4\n",
            " 4 4 3 4 4 4 4 4 4]\n",
            "Actual: [2 1 1 2 5 4 5 1 1 1 4 5 2 5 4 2 5 2 2 4 4 5 2 4 1 1 5 4 2 5 4 3 1 2 2 0 0\n",
            " 2 5 0 0 1 0 0 5 3 0 3 1 5 5 0 2 1 3 1 4 3 5 3 0 0 1 1 0 0 2 3 4 2 3 1 2 0\n",
            " 4 5 1 1 2 1 5 4 2 1 2 2 3 4 3 0 1 3 1 5 5 1 5 4 3 4 0 4 5 5 3 3 0 3 2 5 3\n",
            " 5 5 1 0 2 2 1 1 4 0 4 2 3 5 3 3 4 5 2 5 3 1 0 2 3 2 2 4 2 4 0 5 3 5 4 4 5\n",
            " 4 0 2 0 1 2 0 2 1 5 5 0 0 0 0 4 3 4 5 1 3 2 0 5 4 2 2 4 1 3 2 3 3 4 2 5 4\n",
            " 1 3 4 5 4 0 5 0 2 5 0 3 0 5 4 5 1 5 4 3 4 5 2 4 5 4 1 4 1 3 3 3 2 1 1 4 0\n",
            " 3 3 4 0 1 1 3 2 4 5 4 0 2 5 2 4 3 3 2 5 0 2 0 3 5 2 1 2 1 2 1 0 3 5 5 4 0\n",
            " 2 4 1 4 1 4 0 2 1 1 4 3 1 1 2 1 1 0 3 5 4 1 5 0 0 2 3 3 5 5 4 2 1 0 5 2 5\n",
            " 3 1 1 3 5 5 0 4 5 1 4 2 3 2 3 3 1 0 0 2 3 2 4 5 3 0 4 5 1 1 3 2 4 3 1 3 0\n",
            " 0 4 3 2 0 1 1 0 0 5 2 1 4 4 5 4 3 3 4 3 0 3 2 1 5 2 5 5 1 4 3 4 3 0 3 1 2\n",
            " 3 3 4 1 2 1 1 4 3 4 0 3 5 4 5 2 2 0 0 0 0 0 3 5 3 4 4 2 1 5 0 5 3 0 1 2 1\n",
            " 2 3 3 0 1 3 0 0 4 3 3 5 0 2 4 1 0 4 0 1 2 0 3 3 2 2 0 0 0 2 0 3 5 1 4 4 4\n",
            " 1 3 5 1 5 2 0 1 2 0 5 3 5 2 4 2 2 1 3 0 5 2 2 5 4 5 2 0 2 0 2 5 3 0 4 5 0\n",
            " 0 1 3 4 5 1 4 4 1]\n"
          ]
        }
      ]
    }
  ]
}