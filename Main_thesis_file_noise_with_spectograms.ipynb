{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiteshJindal/Thesis_Audio_Uncertainity/blob/main/Main_thesis_file_noise_with_spectograms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBzx4tLJhTIu"
      },
      "outputs": [],
      "source": [
        "!pip install pydub\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization, GlobalMaxPooling1D, SpatialDropout1D, Flatten, Concatenate, Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlmlkWBmfBPq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xUlcLO-Fhtjq"
      },
      "outputs": [],
      "source": [
        "# Create an empty DataFrame to store the results\n",
        "final_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'phones'])\n",
        "\n",
        "#directory = '/gdrive/MyDrive/Input_large_final/Input_large/textGrid_training_large/'\n",
        "directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/LargeFiles/TextGrid_noise_training'\n",
        "wav_files_directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/LargeFiles/wav_noise_training'\n",
        "\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "    start_time, end_time, label = None, None, None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('xmin'):\n",
        "            start_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('xmax'):\n",
        "            end_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('text'):\n",
        "            label = line.split('=')[1].strip().strip('\"')\n",
        "            if start_time is not None and end_time is not None and label is not None:\n",
        "                data.append((start_time, end_time, label))\n",
        "                start_time, end_time, label = None, None, None\n",
        "\n",
        "    return data\n",
        "\n",
        "def textgrid_to_dataframe(file_path):\n",
        "    data = parse_textgrid(file_path)\n",
        "    df = pd.DataFrame(data, columns=['Start Time', 'End Time', 'Label'])\n",
        "    return df\n",
        "\n",
        "def extract_mfcc_spectrogram(file_path):\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta_delta = librosa.feature.delta(mfcc, order=2)\n",
        "    spectrogram = np.concatenate((mfcc, mfcc_delta, mfcc_delta_delta), axis=0)\n",
        "    return spectrogram\n",
        "\n",
        "\n",
        "def extract_mel_spectrogram(file_path, n_mels=32, hop_length=512):\n",
        "    spectrograms = []\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfcc_spectrogram = extract_mfcc_spectrogram(file_path)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(S=librosa.feature.inverse.mfcc_to_mel(mfcc_spectrogram),\n",
        "                                                     n_mels=n_mels, hop_length=hop_length)\n",
        "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "    spectrograms.append(mel_spectrogram_db)\n",
        "    return np.array(spectrograms)\n",
        "\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.TextGrid'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Process the file and obtain the necessary dataframes\n",
        "        df = textgrid_to_dataframe(file_path)\n",
        "\n",
        "        # Get the indices of the matched rows\n",
        "        indices = df.index[(df['Start Time'] == df['Start Time'].iloc[0]) & (df['End Time'] == df['End Time'].iloc[0])]\n",
        "\n",
        "        # Split the DataFrame based on indices\n",
        "        first_df = df.loc[:indices[-1]]\n",
        "        second_df = df.loc[indices[-1]+1:]\n",
        "\n",
        "        # Remove rows with blank or null labels from first_df\n",
        "        first_df = first_df[first_df['Label'].notnull() & (first_df['Label'] != \"\")]\n",
        "\n",
        "        # Remove rows with blank or null labels from second_df\n",
        "        second_df = second_df[second_df['Label'].notnull() & (second_df['Label'] != \"\")]\n",
        "\n",
        "        # Combine labels from first_df into a single sentence\n",
        "        combined_sentence = ' '.join(first_df['Label'].tolist())\n",
        "\n",
        "        # Create Combined_df with the combined sentence\n",
        "        combined_df = pd.DataFrame({'Transcript': [combined_sentence]})\n",
        "\n",
        "        # Find the highest occurring string in second_df\n",
        "        phoneme_likelihood = second_df['Label'].mode().iloc[0]\n",
        "\n",
        "        # Create Transcript DataFrame with the most probable phoneme\n",
        "        transcript_df = pd.DataFrame({'phoneme_likelihood': [phoneme_likelihood]})\n",
        "\n",
        "        # Create Phones DataFrame with the list of phones\n",
        "        phones_df = pd.DataFrame({'Phones': [second_df['Label'].tolist()]})\n",
        "\n",
        "        # Extract the MFCC mel spectrogram\n",
        "        audio_file_path = os.path.join(wav_files_directory, filename[:-9] + '.wav')\n",
        "        mfcc_spectrogram = extract_mfcc_spectrogram(audio_file_path)\n",
        "\n",
        "        # Extract the Mel spectrograms\n",
        "        mel_spectrograms = extract_mel_spectrogram(audio_file_path)\n",
        "\n",
        "        # Create MFCC Spectrogram DataFrame with the MFCC mel spectrogram\n",
        "        mfcc_spectrogram_df = pd.DataFrame({'MFCC_Spectrogram': [mfcc_spectrogram]})\n",
        "\n",
        "        # Create Mel Spectrogram DataFrame with the mel spectrogram\n",
        "        mel_spectrograms_df = pd.DataFrame({'Mel_Spectrograms': [mel_spectrograms]})\n",
        "\n",
        "        # Concatenate the DataFrames and append to the final_df\n",
        "        result_df = pd.concat([combined_df, transcript_df, phones_df, mfcc_spectrogram_df, mel_spectrograms_df], axis=1)\n",
        "        final_df = pd.concat([final_df, result_df], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYgYO2Tetwgs"
      },
      "outputs": [],
      "source": [
        "final_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N88EqwSfgz9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Create an empty DataFrame to store the results\n",
        "final_test_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'Phones'])\n",
        "\n",
        "directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/LargeFiles/Textgrid_noise_test'\n",
        "wav_test_files_directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/LargeFiles/wav_noise_test'\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "    start_time, end_time, label = None, None, None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('xmin'):\n",
        "            start_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('xmax'):\n",
        "            end_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('text'):\n",
        "            label = line.split('=')[1].strip().strip('\"')\n",
        "            if start_time is not None and end_time is not None and label is not None:\n",
        "                data.append((start_time, end_time, label))\n",
        "                start_time, end_time, label = None, None, None\n",
        "\n",
        "    return data\n",
        "\n",
        "def textgrid_to_dataframe(file_path):\n",
        "    data = parse_textgrid(file_path)\n",
        "    df = pd.DataFrame(data, columns=['Start Time', 'End Time', 'Label'])\n",
        "    return df\n",
        "\n",
        "def extract_mfcc_spectrogram(file_path):\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta_delta = librosa.feature.delta(mfcc, order=2)\n",
        "    spectrogram = np.concatenate((mfcc, mfcc_delta, mfcc_delta_delta), axis=0)\n",
        "    return spectrogram\n",
        "\n",
        "def extract_mel_spectrogram(file_path, n_mels=32, hop_length=512):\n",
        "    spectrograms = []\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfcc_spectrogram = extract_mfcc_spectrogram(file_path)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(S=librosa.feature.inverse.mfcc_to_mel(mfcc_spectrogram),\n",
        "                                                     n_mels=n_mels, hop_length=hop_length)\n",
        "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "    spectrograms.append(mel_spectrogram_db)\n",
        "    return np.array(spectrograms)\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.TextGrid'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Process the file and obtain the necessary dataframes\n",
        "        df = textgrid_to_dataframe(file_path)\n",
        "\n",
        "        # Get the indices of the matched rows\n",
        "        indices = df.index[(df['Start Time'] == df['Start Time'].iloc[0]) & (df['End Time'] == df['End Time'].iloc[0])]\n",
        "\n",
        "        # Split the DataFrame based on indices\n",
        "        first_test_df = df.loc[:indices[-1]]\n",
        "        second_test_df = df.loc[indices[-1]+1:]\n",
        "\n",
        "        # Remove rows with blank or null labels from first_df\n",
        "        first_test_df = first_test_df[first_test_df['Label'].notnull() & (first_test_df['Label'] != \"\")]\n",
        "\n",
        "        # Remove rows with blank or null labels from second_df\n",
        "        second_test_df = second_test_df[second_test_df['Label'].notnull() & (second_test_df['Label'] != \"\")]\n",
        "\n",
        "        # Combine labels from first_df into a single sentence\n",
        "        combined_test_sentence = ' '.join(first_test_df['Label'].tolist())\n",
        "\n",
        "        # Create Combined_df with the combined sentence\n",
        "        combined_test_df = pd.DataFrame({'Transcript': [combined_test_sentence]})\n",
        "\n",
        "        # Find the highest occurring strings in second_df\n",
        "        highest_occurrences = second_test_df['Label'].mode()\n",
        "\n",
        "        if not highest_occurrences.empty:\n",
        "            # Choose a random element from the list of highest occurrences\n",
        "            phoneme_test_likelihood = random.choice(highest_occurrences.tolist())\n",
        "        else:\n",
        "            phoneme_test_likelihood = None  # Handle the case where there are no labels\n",
        "        # Create Transcript DataFrame with the highest occurring string\n",
        "        transcript_test_df = pd.DataFrame({'phoneme_likelihood': [phoneme_test_likelihood]})\n",
        "\n",
        "        # Create Phones DataFrame with the list of phones\n",
        "        phones_test_df = pd.DataFrame({'Phones': [second_test_df['Label'].tolist()]})\n",
        "\n",
        "        # Extract the MFCC mel spectrogram\n",
        "        audio_file_path = os.path.join(wav_test_files_directory, filename[:-9] + '.wav')\n",
        "        mfcc_test_spectogram = extract_mfcc_spectrogram(audio_file_path)\n",
        "\n",
        "        # Extract the Mel spectrograms\n",
        "        mel_test_spectrograms = extract_mel_spectrogram(audio_file_path)\n",
        "\n",
        "        # Create MFCC Spectrogram DataFrame with the MFCC mel spectrogram\n",
        "        mfcc_spectrogram_test_df = pd.DataFrame({'MFCC_Spectrogram': [mfcc_test_spectogram]})\n",
        "\n",
        "        # Create Mel Spectrogram DataFrame with the mel spectrogram\n",
        "        mel_spectrograms_test_df = pd.DataFrame({'Mel_Spectrograms': [mel_test_spectrograms]})\n",
        "\n",
        "        # Concatenate the DataFrames and append to the final_df\n",
        "        result_test_df = pd.concat([combined_test_df, transcript_test_df, phones_test_df, mfcc_spectrogram_test_df, mel_spectrograms_test_df], axis=1)\n",
        "        final_test_df = pd.concat([final_test_df, result_test_df], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYKFAm6FMvJI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization, GlobalMaxPooling1D, SpatialDropout1D, Flatten, Input, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import LSTM, MaxPooling1D\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "X_transcript = final_df['Transcript']\n",
        "X_MFCC_spectrograms = final_df['MFCC_Spectrogram']\n",
        "y = final_df['phoneme_likelihood']\n",
        "\n",
        "#  the data into training and test sets\n",
        "X_train_transcript, X_test_transcript, X_train_MFCC_spectrograms, X_test_MFCC_spectrograms, y_train, y_test = train_test_split(\n",
        "    X_transcript, X_MFCC_spectrograms, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the transcript training data\n",
        "tokenizer.fit_on_texts(X_train_transcript)\n",
        "\n",
        "# Convert transcript text to sequences\n",
        "X_train_transcript_seq = tokenizer.texts_to_sequences(X_train_transcript)\n",
        "X_test_transcript_seq = tokenizer.texts_to_sequences(X_test_transcript)\n",
        "\n",
        "# Pad transcript sequences to have the same length\n",
        "max_length_transcript = max(max(len(seq) for seq in X_train_transcript_seq), max(len(seq) for seq in X_test_transcript_seq))\n",
        "X_train_transcript_padded = pad_sequences(X_train_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "X_test_transcript_padded = pad_sequences(X_test_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "\n",
        "# Find the maximum sequence length for transcript\n",
        "max_length_transcript = max(len(seq) for seq in X_train_transcript_seq + X_test_transcript_seq)\n",
        "\n",
        "# Find the maximum number of features for spectrograms\n",
        "max_features_spectrogram = max(arr.shape[1] for arr in X_train_MFCC_spectrograms + X_test_MFCC_spectrograms)\n",
        "\n",
        "# Pad or truncate the transcript sequences to have the same length\n",
        "X_train_transcript_padded = pad_sequences(X_train_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "X_test_transcript_padded = pad_sequences(X_test_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "\n",
        "# Pad or truncate the spectrogram arrays to have the same number of features\n",
        "X_train_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :max_features_spectrogram], ((0, 0), (0, max_features_spectrogram - arr.shape[1])), mode='constant') for arr in X_train_MFCC_spectrograms])\n",
        "X_test_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :max_features_spectrogram], ((0, 0), (0, max_features_spectrogram - arr.shape[1])), mode='constant') for arr in X_test_MFCC_spectrograms])\n",
        "\n",
        "\n",
        "# Encode the response variable\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y)  # Fit the label encoder on all labels in y\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Find the maximum number of features for spectrograms\n",
        "max_features_spectrogram = max(max(arr.shape[1] for arr in X_train_MFCC_spectrograms), max(arr.shape[1] for arr in X_test_MFCC_spectrograms))\n",
        "\n",
        "# Pad or truncate the spectrogram arrays to have the same number of features\n",
        "X_train_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :777], ((0, 0), (0, 777 - arr.shape[1])), mode='constant') if arr.shape[1] < 777 else arr[:, :777] for arr in X_train_MFCC_spectrograms])\n",
        "X_test_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :777], ((0, 0), (0, 777 - arr.shape[1])), mode='constant') if arr.shape[1] < 777 else arr[:, :777] for arr in X_test_MFCC_spectrograms])\n",
        "\n",
        "# Define the model\n",
        "input_transcript = Input(shape=(max_length_transcript,))\n",
        "embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(input_transcript)\n",
        "conv1d_transcript = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding)\n",
        "conv1d_transcript = BatchNormalization()(conv1d_transcript)\n",
        "conv1d_transcript = GlobalMaxPooling1D()(conv1d_transcript)\n",
        "conv1d_transcript = Dropout(0.5)(conv1d_transcript)\n",
        "dense = Dense(units=128, activation='relu', kernel_regularizer=l2(0.0001))(conv1d_transcript)\n",
        "dense = Dropout(0.5)(dense)\n",
        "\n",
        "# Add LSTM layer to the transcript part\n",
        "lstm_transcript = LSTM(64)(embedding)\n",
        "lstm_transcript = Dropout(0.5)(lstm_transcript)\n",
        "\n",
        "# Merge the LSTM and Conv1D features\n",
        "merged_features = Concatenate()([dense, lstm_transcript])\n",
        "input_spectrogram = Input(shape=(39, max_features_spectrogram))\n",
        "conv1d = Conv1D(filters=128, kernel_size=3, activation='relu')(input_spectrogram)\n",
        "conv1d = MaxPooling1D(pool_size=2)(conv1d)\n",
        "conv1d = BatchNormalization()(conv1d)\n",
        "conv1d = Dropout(0.5)(conv1d)\n",
        "\n",
        "# Add another Conv1D layer\n",
        "conv1d = Conv1D(filters=128, kernel_size=3, activation='relu')(conv1d)\n",
        "conv1d = MaxPooling1D(pool_size=2)(conv1d)\n",
        "conv1d = BatchNormalization()(conv1d)\n",
        "conv1d = Dropout(0.5)(conv1d)\n",
        "\n",
        "# Flatten the Conv1D output\n",
        "conv1d = Flatten()(conv1d)\n",
        "\n",
        "# Merge the transcript and spectrogram features\n",
        "merged_features = Concatenate()([conv1d, merged_features])\n",
        "\n",
        "dense = Dense(units=256, activation='relu')(merged_features)\n",
        "dense = BatchNormalization()(dense)\n",
        "dense = Dropout(0.5)(dense)\n",
        "\n",
        "output = Dense(units=len(label_encoder.classes_), activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=[input_transcript, input_spectrogram], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "#early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_train_transcript_padded, X_train_MFCC_spectrograms_padded], y_train_encoded,\n",
        "          epochs=10, batch_size=16, validation_data=([X_test_transcript_padded, X_test_MFCC_spectrograms_padded], y_test_encoded))\n",
        "\n",
        "# Calculate training and validation accuracy\n",
        "_, train_accuracy = model.evaluate([X_train_transcript_padded, X_train_MFCC_spectrograms_padded], y_train_encoded)\n",
        "_, test_accuracy = model.evaluate([X_test_transcript_padded, X_test_MFCC_spectrograms_padded], y_test_encoded)\n",
        "\n",
        "print('Training Accuracy:', train_accuracy)\n",
        "print('Validation Accuracy:', test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2jIsq2vdAdg"
      },
      "outputs": [],
      "source": [
        "# Extract the input features from the final test data\n",
        "X_new_mfcc_spectrograms = final_test_df['MFCC_Spectrogram']\n",
        "X_new_transcripts = final_test_df['Transcript']\n",
        "\n",
        "# Convert text to sequences using the tokenizer fitted on the training data\n",
        "X_new_transcript_seq = tokenizer.texts_to_sequences(X_new_transcripts)\n",
        "X_new_transcript_padded = pad_sequences(X_new_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "\n",
        "#print(X_new_spectrograms.shape, X_new_spectrograms.dtype)\n",
        "# Make predictions on the new data\n",
        "# Find the maximum number of features for spectrograms\n",
        "\n",
        "\n",
        "# Pad or truncate the spectrogram arrays to have the same number of features\n",
        "X_new_mfcc_spectrograms_padded = np.array([np.pad(arr[:, :777], ((0, 0), (0, 777 - arr.shape[1])), mode='constant') if arr.shape[1] < 777 else arr[:, :777] for arr in X_new_mfcc_spectrograms])\n",
        "predictions = model.predict([X_new_transcript_padded, X_new_mfcc_spectrograms_padded])\n",
        "\n",
        "# Decode the predicted labels\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "print(predicted_labels)\n",
        "\n",
        "# Calculate the accuracy on the new data\n",
        "accuracy = np.mean(predicted_labels == final_test_df['phoneme_likelihood'])\n",
        "print(\"Accuracy on New Data:\", accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}