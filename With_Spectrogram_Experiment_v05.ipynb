{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installed necessary libraries**"
      ],
      "metadata": {
        "id": "DAdwO-jSwPSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "sZtRuqAcwMyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary libraries**"
      ],
      "metadata": {
        "id": "m2QcEisitV5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Operating system interaction\n",
        "import os\n",
        "\n",
        "# Audio processing\n",
        "import librosa\n",
        "\n",
        "# Hyperparameter tuning for Keras models\n",
        "import kerastuner as kt\n",
        "\n",
        "# Deep learning framework\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv1D, Dense, Embedding, LSTM, Bidirectional,\n",
        "    Dropout, BatchNormalization, GlobalMaxPooling1D,\n",
        "    SpatialDropout1D, Flatten, Input, Concatenate, MaxPooling1D\n",
        ")\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Model tuning\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner import HyperParameters\n",
        "\n",
        "# Data preprocessing and balancing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n"
      ],
      "metadata": {
        "id": "fYPuXEXDwFHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drive Mount**"
      ],
      "metadata": {
        "id": "xosDp-4ntZvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "rGbXVtTFfo-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Without Noise Directory**"
      ],
      "metadata": {
        "id": "dYz0Y6yNvvVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/Small_Files_Merged/Textgrid_Files'\n",
        "wav_files_directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/Small_Files_Merged/Wav_Files'"
      ],
      "metadata": {
        "id": "6z_8_Dcxv6ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With Noise Directory**"
      ],
      "metadata": {
        "id": "0SyjJg93v9-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/Small_Files_Merged/Textgrid_Noise_Files'\n",
        "wav_files_directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/Small_Files_Merged/Wav_Noise_Files'"
      ],
      "metadata": {
        "id": "yNIyhOXmv8nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loading and DataFrame Creation**"
      ],
      "metadata": {
        "id": "Rr1LpNcZtv5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create an empty DataFrame to store the results\n",
        "final_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'phones'])\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "    start_time, end_time, label = None, None, None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('xmin'):\n",
        "            start_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('xmax'):\n",
        "            end_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('text'):\n",
        "            label = line.split('=')[1].strip().strip('\"')\n",
        "            if start_time is not None and end_time is not None and label is not None:\n",
        "                data.append((start_time, end_time, label))\n",
        "                start_time, end_time, label = None, None, None\n",
        "\n",
        "    return data\n",
        "\n",
        "def textgrid_to_dataframe(file_path):\n",
        "    data = parse_textgrid(file_path)\n",
        "    df = pd.DataFrame(data, columns=['Start Time', 'End Time', 'Label'])\n",
        "    return df\n",
        "\n",
        "def extract_mfcc_spectrogram(file_path):\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta_delta = librosa.feature.delta(mfcc, order=2)\n",
        "    spectrogram = np.concatenate((mfcc, mfcc_delta, mfcc_delta_delta), axis=0)\n",
        "    return spectrogram\n",
        "\n",
        "\n",
        "def extract_mel_spectrogram(file_path, n_mels=32, hop_length=512):\n",
        "    spectrograms = []\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfcc_spectrogram = extract_mfcc_spectrogram(file_path)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(S=librosa.feature.inverse.mfcc_to_mel(mfcc_spectrogram),\n",
        "                                                     n_mels=n_mels, hop_length=hop_length)\n",
        "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "    spectrograms.append(mel_spectrogram_db)\n",
        "    return np.array(spectrograms)\n",
        "\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.TextGrid'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Process the file and obtain the necessary dataframes\n",
        "        df = textgrid_to_dataframe(file_path)\n",
        "\n",
        "        # Get the indices of the matched rows\n",
        "        indices = df.index[(df['Start Time'] == df['Start Time'].iloc[0]) & (df['End Time'] == df['End Time'].iloc[0])]\n",
        "\n",
        "        # Split the DataFrame based on indices\n",
        "        first_df = df.loc[:indices[-1]]\n",
        "        second_df = df.loc[indices[-1]+1:]\n",
        "\n",
        "        # Remove rows with blank or null labels from first_df\n",
        "        first_df = first_df[first_df['Label'].notnull() & (first_df['Label'] != \"\")]\n",
        "\n",
        "        # Remove rows with blank or null labels from second_df\n",
        "        second_df = second_df[second_df['Label'].notnull() & (second_df['Label'] != \"\")]\n",
        "\n",
        "        # Combine labels from first_df into a single sentence\n",
        "        combined_sentence = ' '.join(first_df['Label'].tolist())\n",
        "\n",
        "        # Create Combined_df with the combined sentence\n",
        "        combined_df = pd.DataFrame({'Transcript': [combined_sentence]})\n",
        "\n",
        "        # Find the highest occurring string in second_df\n",
        "        phoneme_likelihood = second_df['Label'].mode().iloc[0]\n",
        "\n",
        "        # Create Transcript DataFrame with the most probable phoneme\n",
        "        transcript_df = pd.DataFrame({'phoneme_likelihood': [phoneme_likelihood]})\n",
        "\n",
        "        # Create Phones DataFrame with the list of phones\n",
        "        phones_df = pd.DataFrame({'Phones': [second_df['Label'].tolist()]})\n",
        "\n",
        "        # Extract the MFCC mel spectrogram\n",
        "        audio_file_path = os.path.join(wav_files_directory, filename[:-9] + '.wav')\n",
        "        mfcc_spectrogram = extract_mfcc_spectrogram(audio_file_path)\n",
        "\n",
        "        # Extract the Mel spectrograms\n",
        "        mel_spectrograms = extract_mel_spectrogram(audio_file_path)\n",
        "\n",
        "        # Create MFCC Spectrogram DataFrame with the MFCC mel spectrogram\n",
        "        mfcc_spectrogram_df = pd.DataFrame({'MFCC_Spectrogram': [mfcc_spectrogram]})\n",
        "\n",
        "        # Create Mel Spectrogram DataFrame with the mel spectrogram\n",
        "        mel_spectrograms_df = pd.DataFrame({'Mel_Spectrograms': [mel_spectrograms]})\n",
        "\n",
        "        # Concatenate the DataFrames and append to the final_df\n",
        "        result_df = pd.concat([combined_df, transcript_df, phones_df, mfcc_spectrogram_df, mel_spectrograms_df], axis=1)\n",
        "        final_df = pd.concat([final_df, result_df], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "38HNWTUYhyl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Refinement Process**"
      ],
      "metadata": {
        "id": "OLQuObpjt1fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the labels of the 3 most frequent classes\n",
        "phoneme_likelihood_counts = final_df['phoneme_likelihood'].value_counts()\n",
        "most_frequent_classes = phoneme_likelihood_counts.index[:3]\n",
        "\n",
        "# Filter 'final_df' to keep only the observations with labels in the most frequent classes\n",
        "filtered_final_df = final_df[final_df['phoneme_likelihood'].isin(most_frequent_classes)]\n",
        "\n",
        "print(filtered_final_df['phoneme_likelihood'].value_counts())"
      ],
      "metadata": {
        "id": "MANNNonAiREn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Partition: Training and Test Split**"
      ],
      "metadata": {
        "id": "CPojYv7Jt__Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test dataframes (70% training, 30% test), using a fixed random state for reproducibility\n",
        "train_df, test_df = train_test_split(filtered_final_df, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "HUGL8M3yiWOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balancing the Training Data using Random Oversampling**"
      ],
      "metadata": {
        "id": "YZxCTb4UuBw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary dataframe before resampling the data to balance the imbalanced dataset\n",
        "X_temp = train_df\n",
        "X_temp = X_temp.drop('phoneme_likelihood', axis=1)\n",
        "# Oversampling\n",
        "\n",
        "# Calculate the desired minority class count based on 0.5 times the majority class count\n",
        "y = train_df['phoneme_likelihood']\n",
        "majority_class_count = max(Counter(y).values())\n",
        "desired_minority_class_count = int(majority_class_count )  # Changed to 0.25\n",
        "\n",
        "# Prepare the sampling_strategy dictionary\n",
        "sampling_strategy = {\n",
        "    label: desired_minority_class_count\n",
        "    for label, count in Counter(y).items()\n",
        "    if count < desired_minority_class_count\n",
        "}\n",
        "\n",
        "# Initialize RandomOverSampler with the custom sampling strategy\n",
        "oversampler = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=0)\n",
        "\n",
        "# Perform oversampling on the training data\n",
        "X_temp_resampled, y_resampled = oversampler.fit_resample(X_temp, y)\n",
        "\n",
        "# Print class distribution after oversampling\n",
        "print(\"Class distribution after oversampling:\", sorted(Counter(y_resampled).items()))"
      ],
      "metadata": {
        "id": "GojAiWCOiW49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Model Training and Evaluation with MFCC_Spectrogram**"
      ],
      "metadata": {
        "id": "morxYYZkuaLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_transcript = X_temp_resampled['Transcript']\n",
        "X_MFCC_spectrograms = X_temp_resampled['MFCC_Spectrogram']\n",
        "y = y_resampled\n",
        "\n",
        "#  the data into training and test sets\n",
        "X_train_transcript, X_test_transcript, X_train_MFCC_spectrograms, X_test_MFCC_spectrograms, y_train, y_test = train_test_split(\n",
        "    X_transcript, X_MFCC_spectrograms, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the transcript training data\n",
        "tokenizer.fit_on_texts(X_train_transcript)\n",
        "\n",
        "# Convert transcript text to sequences\n",
        "X_train_transcript_seq = tokenizer.texts_to_sequences(X_train_transcript)\n",
        "X_test_transcript_seq = tokenizer.texts_to_sequences(X_test_transcript)\n",
        "\n",
        "# Pad transcript sequences to have the same length\n",
        "max_length_transcript = max(max(len(seq) for seq in X_train_transcript_seq), max(len(seq) for seq in X_test_transcript_seq))\n",
        "X_train_transcript_padded = pad_sequences(X_train_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "X_test_transcript_padded = pad_sequences(X_test_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "\n",
        "# Find the maximum sequence length for transcript\n",
        "max_length_transcript = max(len(seq) for seq in X_train_transcript_seq + X_test_transcript_seq)\n",
        "\n",
        "# Find the maximum number of features for spectrograms\n",
        "max_features_spectrogram = max(arr.shape[1] for arr in X_train_MFCC_spectrograms + X_test_MFCC_spectrograms)\n",
        "\n",
        "# Pad or truncate the transcript sequences to have the same length\n",
        "X_train_transcript_padded = pad_sequences(X_train_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "X_test_transcript_padded = pad_sequences(X_test_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "\n",
        "# Pad or truncate the spectrogram arrays to have the same number of features\n",
        "X_train_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :max_features_spectrogram], ((0, 0), (0, max_features_spectrogram - arr.shape[1])), mode='constant') for arr in X_train_MFCC_spectrograms])\n",
        "X_test_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :max_features_spectrogram], ((0, 0), (0, max_features_spectrogram - arr.shape[1])), mode='constant') for arr in X_test_MFCC_spectrograms])\n",
        "\n",
        "\n",
        "# Encode the response variable\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y)  # Fit the label encoder on all labels in y\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Find the maximum number of features for spectrograms\n",
        "max_features_spectrogram = max(max(arr.shape[1] for arr in X_train_MFCC_spectrograms), max(arr.shape[1] for arr in X_test_MFCC_spectrograms))\n",
        "\n",
        "# Pad or truncate the spectrogram arrays to have the same number of features\n",
        "X_train_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :max_features_spectrogram], ((0, 0), (0, max_features_spectrogram - arr.shape[1])), mode='constant') for arr in X_train_MFCC_spectrograms])\n",
        "X_test_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :max_features_spectrogram], ((0, 0), (0, max_features_spectrogram - arr.shape[1])), mode='constant') for arr in X_test_MFCC_spectrograms])\n",
        "# Define the model\n",
        "input_transcript = Input(shape=(max_length_transcript,))\n",
        "embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(input_transcript)\n",
        "conv1d_transcript = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding)\n",
        "conv1d_transcript = BatchNormalization()(conv1d_transcript)\n",
        "conv1d_transcript = GlobalMaxPooling1D()(conv1d_transcript)\n",
        "conv1d_transcript = Dropout(0.5)(conv1d_transcript)\n",
        "dense = Dense(units=128, activation='relu', kernel_regularizer=l2(0.0001))(conv1d_transcript)\n",
        "dense = Dropout(0.5)(dense)\n",
        "\n",
        "# Add LSTM layer to the transcript part\n",
        "lstm_transcript = LSTM(64)(embedding)\n",
        "lstm_transcript = Dropout(0.5)(lstm_transcript)\n",
        "\n",
        "# Merge the LSTM and Conv1D features\n",
        "merged_features = Concatenate()([dense, lstm_transcript])\n",
        "input_spectrogram = Input(shape=(39, max_features_spectrogram))\n",
        "conv1d = Conv1D(filters=128, kernel_size=5, activation='relu')(input_spectrogram)\n",
        "conv1d = GlobalMaxPooling1D()(conv1d)\n",
        "conv1d = BatchNormalization()(conv1d)\n",
        "conv1d = Dropout(0.5)(conv1d)\n",
        "\n",
        "# Flatten the Conv1D output\n",
        "conv1d = Flatten()(conv1d)\n",
        "\n",
        "# Merge the transcript and spectrogram features\n",
        "merged_features = Concatenate()([conv1d, merged_features])\n",
        "\n",
        "dense = Dense(units=256, activation='relu')(merged_features)\n",
        "dense = BatchNormalization()(dense)\n",
        "dense = Dropout(0.5)(dense)\n",
        "\n",
        "output = Dense(units=len(label_encoder.classes_), activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=[input_transcript, input_spectrogram], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "#early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_train_transcript_padded, X_train_MFCC_spectrograms_padded], y_train_encoded,\n",
        "          epochs=10, batch_size=64, validation_data=([X_test_transcript_padded, X_test_MFCC_spectrograms_padded], y_test_encoded))\n",
        "\n",
        "# Calculate training and validation accuracy\n",
        "_, train_accuracy = model.evaluate([X_train_transcript_padded, X_train_MFCC_spectrograms_padded], y_train_encoded)\n",
        "_, test_accuracy = model.evaluate([X_test_transcript_padded, X_test_MFCC_spectrograms_padded], y_test_encoded)\n",
        "\n",
        "print('Training Accuracy:', train_accuracy)\n",
        "print('Validation Accuracy:', test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "ecXSRdDKiek4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating Accuracy and Predicted Labels on Test Data**"
      ],
      "metadata": {
        "id": "1X51zIQqu5Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the input features from the new data\n",
        "X_new_mfcc_spectrograms = test_df['MFCC_Spectrogram']\n",
        "X_new_transcripts = test_df['Transcript']\n",
        "\n",
        "# Convert text to sequences using the tokenizer fitted on the training data\n",
        "X_new_transcript_seq = tokenizer.texts_to_sequences(X_new_transcripts)\n",
        "X_new_transcript_padded = pad_sequences(X_new_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "# Pad or truncate the spectrogram arrays to have the same number of features\n",
        "X_new_mfcc_spectrograms_padded = np.array([np.pad(arr[:, :max_features_spectrogram], ((0, 0), (0, max_features_spectrogram - arr.shape[1])), mode='constant') for arr in X_new_mfcc_spectrograms])\n",
        "predictions = model.predict([X_new_transcript_padded, X_new_mfcc_spectrograms_padded])\n",
        "\n",
        "# Decode the predicted labels\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "print(predicted_labels)\n",
        "\n",
        "# Calculate the accuracy on the new data\n",
        "accuracy = np.mean(predicted_labels == test_df['phoneme_likelihood'])\n",
        "print(\"Overall Accuracy on Test Data:\", accuracy)"
      ],
      "metadata": {
        "id": "KowwYMP9jCWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating Accuracy and Correct Predictions per Class**"
      ],
      "metadata": {
        "id": "DW_BIe9kvFJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new_mfcc_spectrograms = test_df['MFCC_Spectrogram']\n",
        "X_new_transcripts = test_df['Transcript']\n",
        "\n",
        "# Convert text to sequences using the tokenizer fitted on the training data\n",
        "X_new_transcript_seq = tokenizer.texts_to_sequences(X_new_transcripts)\n",
        "X_new_transcript_padded = pad_sequences(X_new_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "\n",
        "# Pad or truncate the spectrogram arrays to have the same number of features\n",
        "X_new_mfcc_spectrograms_padded = np.array([np.pad(arr[:, :max_features_spectrogram], ((0, 0), (0, max_features_spectrogram - arr.shape[1])), mode='constant') if arr.shape[1] < max_features_spectrogram else arr[:, :max_features_spectrogram] for arr in X_new_mfcc_spectrograms])\n",
        "predictions = model.predict([X_new_transcript_padded, X_new_mfcc_spectrograms_padded])\n",
        "\n",
        "\n",
        "# Decode the predicted labels\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "# Calculate the accuracy on the new data\n",
        "accuracy = accuracy_score(predicted_labels, test_df['phoneme_likelihood'])\n",
        "print(\"Accuracy on New Data:\", accuracy)\n",
        "\n",
        "# Assuming 'predictions' contains the predicted probabilities for each class\n",
        "class_probabilities = predictions / np.sum(predictions, axis=1, keepdims=True)\n",
        "class_probabilities = np.clip(class_probabilities, 1e-10, 1.0 - 1e-10)  # Add a small constant to avoid zero probabilities\n",
        "entropy_per_sample = -np.sum(class_probabilities * np.log2(class_probabilities), axis=1)\n",
        "\n",
        "# Normalize entropy values to be between 0 and 1\n",
        "normalized_entropy = entropy_per_sample / np.log2(len(label_encoder.classes_))\n",
        "\n",
        "# Calculate average normalized entropy per class\n",
        "average_normalized_entropy_per_class = {}\n",
        "\n",
        "for label_idx, label in enumerate(label_encoder.classes_):\n",
        "    indices_for_label = np.where(test_df['phoneme_likelihood'] == label)[0]\n",
        "    uncertainties_for_label = normalized_entropy[indices_for_label]\n",
        "    average_uncertainty = np.mean(uncertainties_for_label)\n",
        "    average_normalized_entropy_per_class[label] = average_uncertainty\n",
        "\n",
        "print(\"Average Normalized Uncertainty per Class Label:\")\n",
        "for label, uncertainty in average_normalized_entropy_per_class.items():\n",
        "    print(f\"{label}: {uncertainty:.4f}\")\n",
        "\n",
        "# Calculate accuracy per class and number of correct predictions\n",
        "class_labels = label_encoder.classes_\n",
        "class_accuracy = {}\n",
        "class_correct_predictions = {}\n",
        "\n",
        "for label in class_labels:\n",
        "    indices_for_label = np.where(test_df['phoneme_likelihood'] == label)[0]\n",
        "    correct_predictions_for_label = np.sum(predicted_labels[indices_for_label] == label)\n",
        "    total_samples_for_label = len(indices_for_label)\n",
        "    class_accuracy[label] = correct_predictions_for_label / total_samples_for_label\n",
        "    class_correct_predictions[label] = correct_predictions_for_label\n",
        "\n",
        "print(\"Accuracy and Correct Predictions per Class:\")\n",
        "for label, acc in class_accuracy.items():\n",
        "    correct_preds = class_correct_predictions[label]\n",
        "    print(f\"{label}: Accuracy = {acc:.4f}, Correct Predictions = {correct_preds}\")\n"
      ],
      "metadata": {
        "id": "MV_LStgyjbkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of Other Metrics Calculation**"
      ],
      "metadata": {
        "id": "q21c5X2ivaqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse transform predicted labels from one-hot encoded format to original labels\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "# Extract the true labels from the 'phoneme_likelihood' column of the test dataframe\n",
        "true_labels = test_df['phoneme_likelihood']\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Print the calculated evaluation metrics\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "id": "u1T0tgxEjjXv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}