{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiteshJindal/Thesis_Audio_Uncertainity/blob/main/NoSpectrogram_NoNoise_Experiment_27_Aug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary libraries**"
      ],
      "metadata": {
        "id": "0nRS_3RqgKZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JB4NRBv5NOoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e41582f-de22-4bf4-ba24-f5f51bc8a861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install pydub\n",
        "!pip install keras-tuner\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "import kerastuner as kt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv1D, Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization, GlobalMaxPooling1D, SpatialDropout1D, Flatten, Concatenate, Input\n",
        "from collections import Counter\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Import necessary functions from sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
        "\n",
        "# Install necessary packages for over-sampling\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drive Mount**"
      ],
      "metadata": {
        "id": "3-0t-VYqqr3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4BqXvq_Pwez",
        "outputId": "4a396a71-9106-4323-ae60-4e45afb8363a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Without Noise - directory path**"
      ],
      "metadata": {
        "id": "OD7lXq4bfRga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/Small_Files_Merged/Textgrid_Files'"
      ],
      "metadata": {
        "id": "eKcECztBfQqJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With Noise - directory path**"
      ],
      "metadata": {
        "id": "ZsOwjLcwfZMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/Small_Files_Merged/Textgrid_noise_Files'"
      ],
      "metadata": {
        "id": "8kkZSKUTfuNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loading and DataFrame Creation**\n",
        "\n"
      ],
      "metadata": {
        "id": "SvMBEs_TgsEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty DataFrame to store the results\n",
        "final_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'Phones'])\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "    start_time, end_time, label = None, None, None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('xmin'):\n",
        "            start_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('xmax'):\n",
        "            end_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('text'):\n",
        "            label = line.split('=')[1].strip().strip('\"')\n",
        "            if start_time is not None and end_time is not None and label is not None:\n",
        "                data.append((start_time, end_time, label))\n",
        "                start_time, end_time, label = None, None, None\n",
        "\n",
        "    return data\n",
        "\n",
        "def textgrid_to_dataframe(file_path):\n",
        "    data = parse_textgrid(file_path)\n",
        "    df = pd.DataFrame(data, columns=['Start Time', 'End Time', 'Label'])\n",
        "    return df\n",
        "\n",
        "# Create an empty DataFrame to store the results\n",
        "final_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'Phones'])\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.TextGrid'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Process the file and obtain the necessary dataframes\n",
        "        df = textgrid_to_dataframe(file_path)\n",
        "\n",
        "        # Get the indices of the matched rows\n",
        "        indices = df.index[(df['Start Time'] == df['Start Time'].iloc[0]) & (df['End Time'] == df['End Time'].iloc[0])]\n",
        "\n",
        "        # Split the DataFrame based on indices\n",
        "        first_df = df.loc[:indices[-1]]\n",
        "        second_df = df.loc[indices[-1]+1:]\n",
        "\n",
        "        # Remove rows with blank or null labels from first_df\n",
        "        first_df = first_df[first_df['Label'].notnull() & (first_df['Label'] != \"\")]\n",
        "\n",
        "        # Remove rows with blank or null labels from second_df\n",
        "        second_df = second_df[second_df['Label'].notnull() & (second_df['Label'] != \"\")]\n",
        "\n",
        "        # Combine labels from first_df into a single sentence\n",
        "        combined_sentence = ' '.join(first_df['Label'].tolist())\n",
        "\n",
        "        # Create Combined_df with the combined sentence\n",
        "        combined_df = pd.DataFrame({'Transcript': [combined_sentence]})\n",
        "\n",
        "        # Find the highest occurring string in second_df\n",
        "        phoneme_likelihood = second_df['Label'].mode().iloc[0]\n",
        "\n",
        "        # Create Transcript DataFrame with the highest occurring string\n",
        "        transcript_df = pd.DataFrame({'phoneme_likelihood': [phoneme_likelihood]})\n",
        "\n",
        "        # Combine labels from second_df into a list\n",
        "        phones_list = second_df['Label'].tolist()\n",
        "\n",
        "        # Create Phones DataFrame with the list of phones\n",
        "        phones_df = pd.DataFrame({'Phones': [phones_list]})\n",
        "\n",
        "        # Concatenate the DataFrames and append to the final_df\n",
        "        result_df = pd.concat([combined_df, transcript_df, phones_df], axis=1)\n",
        "        final_df = pd.concat([final_df, result_df], ignore_index=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "JhMXCZwqQAIz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Refinement Process**"
      ],
      "metadata": {
        "id": "XSEXTDL7hESt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the labels of the 3 most frequent classes\n",
        "phoneme_likelihood_counts = final_df['phoneme_likelihood'].value_counts()\n",
        "most_frequent_classes = phoneme_likelihood_counts.index[:3]\n",
        "\n",
        "# Filter 'final_df' to keep only the observations with labels in the most frequent classes\n",
        "filtered_final_df = final_df[final_df['phoneme_likelihood'].isin(most_frequent_classes)]\n",
        "\n",
        "print(filtered_final_df['phoneme_likelihood'].value_counts())"
      ],
      "metadata": {
        "id": "DymauPjIQWTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Partition: Training and Test Split**"
      ],
      "metadata": {
        "id": "BBkDq9SIhO4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test dataframes (70% training, 30% test), using a fixed random state for reproducibility\n",
        "train_df, test_df = train_test_split(filtered_final_df, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "jZ3ULSa8RLbL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balancing the Training Data using Oversampling**"
      ],
      "metadata": {
        "id": "a_rZ9RUGhtAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary dataframe to hold data before resampling, aiming to balance an imbalanced dataset\n",
        "X_temp = train_df\n",
        "X_temp = X_temp.drop('phoneme_likelihood', axis=1)\n",
        "\n",
        "# Oversampling\n",
        "\n",
        "# Calculate the desired count of the minority class based on 0.5 times the majority class count\n",
        "y = train_df['phoneme_likelihood']\n",
        "majority_class_count = max(Counter(y).values())\n",
        "desired_minority_class_count = int(majority_class_count * 0.25)  # Adjusted to 25% of majority count\n",
        "\n",
        "# Prepare a dictionary for sampling strategy where minority classes will be upsampled\n",
        "sampling_strategy = {\n",
        "    label: desired_minority_class_count\n",
        "    for label, count in Counter(y).items()\n",
        "    if count < desired_minority_class_count\n",
        "}\n",
        "\n",
        "# Initialize the RandomOverSampler with the custom sampling strategy and a fixed random state\n",
        "oversampler = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=0)\n",
        "\n",
        "# Apply oversampling to the training data to balance class distribution\n",
        "X_temp_resampled, y_resampled = oversampler.fit_resample(X_temp, y)\n",
        "\n",
        "# Print the distribution of classes after oversampling\n",
        "print(\"Class distribution after oversampling:\", sorted(Counter(y_resampled).items()))\n"
      ],
      "metadata": {
        "id": "kM9gwsi5RQl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Model Training and Evaluation**\n",
        "\n"
      ],
      "metadata": {
        "id": "lPq0R5RCkCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the input features and response variable\n",
        "X = X_temp_resampled['Transcript']\n",
        "y = y_resampled\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the training data\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_test_seq))\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Encode the response variable\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y)\n",
        "y_encoded = label_encoder.transform(y)\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define the improved model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=200, input_length=max_length))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Define the model checkpoint callback to save the best model during training\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_padded, y_train_encoded, epochs=50, batch_size=64, validation_data=(X_test_padded, y_test_encoded), callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Calculate training and validation accuracy\n",
        "train_accuracy = model.evaluate(X_train_padded, y_train_encoded)[1]\n",
        "val_accuracy = model.evaluate(X_test_padded, y_test_encoded)[1]\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n"
      ],
      "metadata": {
        "id": "AYqKR-XpR6s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Model Training and Evaluation using k-fold cross-validation**"
      ],
      "metadata": {
        "id": "U87_uGj6k7vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize k-fold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store training and validation accuracies\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Iterate through each fold\n",
        "for train_index, val_index in kfold.split(X_temp_resampled):\n",
        "    X_train, X_val = X_temp_resampled.iloc[train_index], X_temp_resampled.iloc[val_index]\n",
        "    y_train, y_val = y_resampled[train_index], y_resampled[val_index]\n",
        "\n",
        "    # Tokenize and preprocess text data\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_train['Transcript'])\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train['Transcript'])\n",
        "    X_val_seq = tokenizer.texts_to_sequences(X_val['Transcript'])\n",
        "    max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_val_seq))\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "    X_val_padded = pad_sequences(X_val_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Encode the response variable\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(y_train)\n",
        "    y_train_encoded = label_encoder.transform(y_train)\n",
        "    y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "    # Define the model architecture\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=200, input_length=max_length))\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "    # Define EarlyStopping and ModelCheckpoint callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "    # Train the model with k-fold cross-validation\n",
        "    history = model.fit(X_train_padded, y_train_encoded, epochs=100, batch_size=64,\n",
        "                        validation_data=(X_val_padded, y_val_encoded),\n",
        "                        callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "    # Calculate training and validation accuracy\n",
        "    train_accuracy = model.evaluate(X_train_padded, y_train_encoded)[1]\n",
        "    val_accuracy = model.evaluate(X_val_padded, y_val_encoded)[1]\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "# Print average training and validation accuracies across folds\n",
        "print(\"Average Training Accuracy:\", np.mean(train_accuracies))\n",
        "print(\"Average Validation Accuracy:\", np.mean(val_accuracies))\n"
      ],
      "metadata": {
        "id": "uyi6zmckU4SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Model Training and Evaluation using both k-fold cross validation and Keras Tuner**"
      ],
      "metadata": {
        "id": "dIklTd1ulK11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize k-fold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store validation accuracies from each fold\n",
        "val_accuracies = []\n",
        "\n",
        "for fold_index, (train_index, val_index) in enumerate(kfold.split(X_temp_resampled)):\n",
        "    X_train, X_val = X_temp_resampled.iloc[train_index], X_temp_resampled.iloc[val_index]\n",
        "    y_train, y_val = y_resampled[train_index], y_resampled[val_index]\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "\n",
        "    # Fit the tokenizer on the training data\n",
        "    tokenizer.fit_on_texts(X_train['Transcript'])\n",
        "\n",
        "    # Convert text to sequences\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train['Transcript'])\n",
        "    X_val_seq = tokenizer.texts_to_sequences(X_val['Transcript'])\n",
        "\n",
        "    # Pad sequences to have the same length\n",
        "    max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_val_seq))\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "    X_val_padded = pad_sequences(X_val_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Encode the response variable\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(y_train)  # Fit the label encoder on train labels\n",
        "    y_train_encoded = label_encoder.transform(y_train)\n",
        "    y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "    # Define the improved model using Keras Tuner\n",
        "    def build_model(hp):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=hp.Int('embedding_dim', min_value=32, max_value=256, step=32), input_length=max_length))\n",
        "        model.add(Conv1D(filters=hp.Int('filters', min_value=32, max_value=256, step=32), kernel_size=5, activation='relu'))\n",
        "        model.add(GlobalMaxPooling1D())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)))\n",
        "        # Add another Conv1D layer\n",
        "        model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "        model.add(Dropout(hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)))\n",
        "        model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # Initialize the tuner\n",
        "    tuner = RandomSearch(\n",
        "        build_model,\n",
        "        objective='val_accuracy',\n",
        "        max_trials=10,\n",
        "        directory=f'tuner_results_fold_{fold_index + 1}',  # Path to store the results for each fold\n",
        "        project_name='phoneme_tuning')  # A unique project name\n",
        "\n",
        "    # Perform hyperparameter search\n",
        "    tuner.search(X_train_padded, y_train_encoded, epochs=50, batch_size=64, validation_data=(X_val_padded, y_val_encoded))\n",
        "\n",
        "    # Get the best model\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    # Calculate validation accuracy for the best model\n",
        "    val_accuracy = best_model.evaluate(X_val_padded, y_val_encoded)[1]\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "# Print average validation accuracy across folds\n",
        "print(\"Average Validation Accuracy:\", np.mean(val_accuracies))"
      ],
      "metadata": {
        "id": "escl44Qmj18P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating Accuracy and Predicted Labels on Test Data**"
      ],
      "metadata": {
        "id": "PfRzFa-om8eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the input features (transcripts) from the test DataFrame\n",
        "X_new = test_df['Transcript']\n",
        "\n",
        "# Convert text to sequences using the tokenizer that was fitted on the training data\n",
        "X_new_seq = tokenizer.texts_to_sequences(X_new)\n",
        "\n",
        "# Pad sequences to have the same length as the sequences in the training data\n",
        "X_new_padded = pad_sequences(X_new_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Make predictions on the new data using the trained model\n",
        "predictions = model.predict(X_new_padded)\n",
        "\n",
        "# Decode the predicted labels using the label encoder\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "# Print the predicted labels\n",
        "print(predicted_labels)\n",
        "\n",
        "# Calculate the accuracy on the new data by comparing predicted labels with true labels\n",
        "accuracy = np.mean(predicted_labels == test_df['phoneme_likelihood'])\n",
        "\n",
        "# Print the accuracy on the new data\n",
        "print(\"Accuracy on New Data:\", accuracy)"
      ],
      "metadata": {
        "id": "B6UTO2-JRoca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating Accuracy and Correct Predictions per Class**"
      ],
      "metadata": {
        "id": "GfMJqvgBoQeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the input features from the new data\n",
        "X_new = test_df['Transcript']\n",
        "\n",
        "# Convert text to sequences using the tokenizer fitted on the training data\n",
        "X_new_seq = tokenizer.texts_to_sequences(X_new)\n",
        "\n",
        "# Pad sequences to have the same length as the training data\n",
        "X_new_padded = pad_sequences(X_new_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Make predictions on the new data\n",
        "predictions = model.predict(X_new_padded)\n",
        "\n",
        "# Decode the predicted labels\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "# Calculate the accuracy on the new data\n",
        "accuracy = accuracy_score(predicted_labels, test_df['phoneme_likelihood'])\n",
        "print(\"Accuracy on New Data:\", accuracy)\n",
        "\n",
        "# Calculate class probabilities from predicted probabilities\n",
        "class_probabilities = predictions / np.sum(predictions, axis=1, keepdims=True)\n",
        "class_probabilities = np.clip(class_probabilities, 1e-10, 1.0 - 1e-10)  # Avoid zero probabilities\n",
        "entropy_per_sample = -np.sum(class_probabilities * np.log2(class_probabilities), axis=1)\n",
        "\n",
        "# Normalize entropy values to be between 0 and 1\n",
        "normalized_entropy = entropy_per_sample / np.log2(len(label_encoder.classes_))\n",
        "\n",
        "# Calculate average normalized entropy per class\n",
        "average_normalized_entropy_per_class = {}\n",
        "\n",
        "for label_idx, label in enumerate(label_encoder.classes_):\n",
        "    indices_for_label = np.where(test_df['phoneme_likelihood'] == label)[0]\n",
        "    uncertainties_for_label = normalized_entropy[indices_for_label]\n",
        "    average_uncertainty = np.mean(uncertainties_for_label)\n",
        "    average_normalized_entropy_per_class[label] = average_uncertainty\n",
        "\n",
        "print(\"Average Normalized Uncertainty per Class Label:\")\n",
        "for label, uncertainty in average_normalized_entropy_per_class.items():\n",
        "    print(f\"{label}: {uncertainty:.4f}\")\n",
        "\n",
        "# Calculate accuracy per class and number of correct predictions\n",
        "class_labels = label_encoder.classes_\n",
        "class_accuracy = {}\n",
        "class_correct_predictions = {}\n",
        "\n",
        "for label in class_labels:\n",
        "    indices_for_label = np.where(test_df['phoneme_likelihood'] == label)[0]\n",
        "    correct_predictions_for_label = np.sum(predicted_labels[indices_for_label] == label)\n",
        "    total_samples_for_label = len(indices_for_label)\n",
        "    class_accuracy[label] = correct_predictions_for_label / total_samples_for_label\n",
        "    class_correct_predictions[label] = correct_predictions_for_label\n",
        "\n",
        "print(\"Accuracy and Correct Predictions per Class:\")\n",
        "for label, acc in class_accuracy.items():\n",
        "    correct_preds = class_correct_predictions[label]\n",
        "    print(f\"{label}: Accuracy = {acc:.4f}, Correct Predictions = {correct_preds}\")"
      ],
      "metadata": {
        "id": "RT783w-sWY-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of Metrics Calculation**"
      ],
      "metadata": {
        "id": "Mui80TKCpIpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse transform predicted labels from one-hot encoded format to original labels\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "# Extract the true labels from the 'phoneme_likelihood' column of the test dataframe\n",
        "true_labels = test_df['phoneme_likelihood']\n",
        "\n",
        "# Calculate precision, recall, and F1-score using the predicted and true labels\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')  # Calculate weighted precision\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')  # Calculate weighted recall\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')  # Calculate weighted F1-score\n",
        "\n",
        "# Print the calculated evaluation metrics\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "id": "PhFBLKB1W_Hm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}