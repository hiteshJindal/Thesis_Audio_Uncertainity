{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiteshJindal/Thesis_Audio_Uncertainity/blob/main/Main_thesis_file_with_spectograms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU2T1VmZi0Kv",
        "outputId": "8e9a0646-9b08-4cde-8909-defd65492bd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-def5c7e0b830>:7: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  import kerastuner as kt\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub\n",
        "!pip install transformers\n",
        "!pip install keras-tuner\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import kerastuner as kt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization, GlobalMaxPooling1D, SpatialDropout1D, Flatten, Concatenate, Input\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization, GlobalMaxPooling1D, SpatialDropout1D, Flatten, Input, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi8WR753i3Ey",
        "outputId": "084e571f-9b88-4217-ca36-96f3405d0adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FlUJPUusi53t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Create an empty DataFrame to store the results\n",
        "final_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'phones'])\n",
        "\n",
        "#directory = '/gdrive/MyDrive/Input_large_final/Input_large/textGrid_training_large/'\n",
        "directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/LargeFiles/TextGrid_noise_training'\n",
        "wav_files_directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/LargeFiles/wav_noise_training'\n",
        "\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "    start_time, end_time, label = None, None, None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('xmin'):\n",
        "            start_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('xmax'):\n",
        "            end_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('text'):\n",
        "            label = line.split('=')[1].strip().strip('\"')\n",
        "            if start_time is not None and end_time is not None and label is not None:\n",
        "                data.append((start_time, end_time, label))\n",
        "                start_time, end_time, label = None, None, None\n",
        "\n",
        "    return data\n",
        "\n",
        "def textgrid_to_dataframe(file_path):\n",
        "    data = parse_textgrid(file_path)\n",
        "    df = pd.DataFrame(data, columns=['Start Time', 'End Time', 'Label'])\n",
        "    return df\n",
        "\n",
        "def extract_mfcc_spectrogram(file_path):\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta_delta = librosa.feature.delta(mfcc, order=2)\n",
        "    spectrogram = np.concatenate((mfcc, mfcc_delta, mfcc_delta_delta), axis=0)\n",
        "    return spectrogram\n",
        "\n",
        "\n",
        "def extract_mel_spectrogram(file_path, n_mels=32, hop_length=512):\n",
        "    spectrograms = []\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfcc_spectrogram = extract_mfcc_spectrogram(file_path)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(S=librosa.feature.inverse.mfcc_to_mel(mfcc_spectrogram),\n",
        "                                                     n_mels=n_mels, hop_length=hop_length)\n",
        "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "    spectrograms.append(mel_spectrogram_db)\n",
        "    return np.array(spectrograms)\n",
        "\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.TextGrid'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Process the file and obtain the necessary dataframes\n",
        "        df = textgrid_to_dataframe(file_path)\n",
        "\n",
        "        # Get the indices of the matched rows\n",
        "        indices = df.index[(df['Start Time'] == df['Start Time'].iloc[0]) & (df['End Time'] == df['End Time'].iloc[0])]\n",
        "\n",
        "        # Split the DataFrame based on indices\n",
        "        first_df = df.loc[:indices[-1]]\n",
        "        second_df = df.loc[indices[-1]+1:]\n",
        "\n",
        "        # Remove rows with blank or null labels from first_df\n",
        "        first_df = first_df[first_df['Label'].notnull() & (first_df['Label'] != \"\")]\n",
        "\n",
        "        # Remove rows with blank or null labels from second_df\n",
        "        second_df = second_df[second_df['Label'].notnull() & (second_df['Label'] != \"\")]\n",
        "\n",
        "        # Combine labels from first_df into a single sentence\n",
        "        combined_sentence = ' '.join(first_df['Label'].tolist())\n",
        "\n",
        "        # Create Combined_df with the combined sentence\n",
        "        combined_df = pd.DataFrame({'Transcript': [combined_sentence]})\n",
        "\n",
        "        # Find the highest occurring string in second_df\n",
        "        phoneme_likelihood = second_df['Label'].mode().iloc[0]\n",
        "\n",
        "        # Create Transcript DataFrame with the most probable phoneme\n",
        "        transcript_df = pd.DataFrame({'phoneme_likelihood': [phoneme_likelihood]})\n",
        "\n",
        "        # Create Phones DataFrame with the list of phones\n",
        "        phones_df = pd.DataFrame({'Phones': [second_df['Label'].tolist()]})\n",
        "\n",
        "        # Extract the MFCC mel spectrogram\n",
        "        audio_file_path = os.path.join(wav_files_directory, filename[:-9] + '.wav')\n",
        "        mfcc_spectrogram = extract_mfcc_spectrogram(audio_file_path)\n",
        "\n",
        "        # Extract the Mel spectrograms\n",
        "        mel_spectrograms = extract_mel_spectrogram(audio_file_path)\n",
        "\n",
        "        # Create MFCC Spectrogram DataFrame with the MFCC mel spectrogram\n",
        "        mfcc_spectrogram_df = pd.DataFrame({'MFCC_Spectrogram': [mfcc_spectrogram]})\n",
        "\n",
        "        # Create Mel Spectrogram DataFrame with the mel spectrogram\n",
        "        mel_spectrograms_df = pd.DataFrame({'Mel_Spectrograms': [mel_spectrograms]})\n",
        "\n",
        "        # Concatenate the DataFrames and append to the final_df\n",
        "        result_df = pd.concat([combined_df, transcript_df, phones_df, mfcc_spectrogram_df, mel_spectrograms_df], axis=1)\n",
        "        final_df = pd.concat([final_df, result_df], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0mYDejGjUcs",
        "outputId": "acb26ab6-8b14-444f-de67-8545a0b9c24b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14473, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "final_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F0tYVVCZjVOP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Create an empty DataFrame to store the results\n",
        "final_test_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'Phones'])\n",
        "\n",
        "directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/LargeFiles/Textgrid_noise_test/'\n",
        "wav_test_files_directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/LargeFiles/wav_noise_test/'\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "    start_time, end_time, label = None, None, None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('xmin'):\n",
        "            start_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('xmax'):\n",
        "            end_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('text'):\n",
        "            label = line.split('=')[1].strip().strip('\"')\n",
        "            if start_time is not None and end_time is not None and label is not None:\n",
        "                data.append((start_time, end_time, label))\n",
        "                start_time, end_time, label = None, None, None\n",
        "\n",
        "    return data\n",
        "\n",
        "def textgrid_to_dataframe(file_path):\n",
        "    data = parse_textgrid(file_path)\n",
        "    df = pd.DataFrame(data, columns=['Start Time', 'End Time', 'Label'])\n",
        "    return df\n",
        "\n",
        "def extract_mfcc_spectrogram(file_path):\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta_delta = librosa.feature.delta(mfcc, order=2)\n",
        "    spectrogram = np.concatenate((mfcc, mfcc_delta, mfcc_delta_delta), axis=0)\n",
        "    return spectrogram\n",
        "\n",
        "def extract_mel_spectrogram(file_path, n_mels=32, hop_length=512):\n",
        "    spectrograms = []\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    mfcc_spectrogram = extract_mfcc_spectrogram(file_path)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(S=librosa.feature.inverse.mfcc_to_mel(mfcc_spectrogram),\n",
        "                                                     n_mels=n_mels, hop_length=hop_length)\n",
        "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "    spectrograms.append(mel_spectrogram_db)\n",
        "    return np.array(spectrograms)\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.TextGrid'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Process the file and obtain the necessary dataframes\n",
        "        df = textgrid_to_dataframe(file_path)\n",
        "\n",
        "        # Get the indices of the matched rows\n",
        "        indices = df.index[(df['Start Time'] == df['Start Time'].iloc[0]) & (df['End Time'] == df['End Time'].iloc[0])]\n",
        "\n",
        "        # Split the DataFrame based on indices\n",
        "        first_test_df = df.loc[:indices[-1]]\n",
        "        second_test_df = df.loc[indices[-1]+1:]\n",
        "\n",
        "        # Remove rows with blank or null labels from first_df\n",
        "        first_test_df = first_test_df[first_test_df['Label'].notnull() & (first_test_df['Label'] != \"\")]\n",
        "\n",
        "        # Remove rows with blank or null labels from second_df\n",
        "        second_test_df = second_test_df[second_test_df['Label'].notnull() & (second_test_df['Label'] != \"\")]\n",
        "\n",
        "        # Combine labels from first_df into a single sentence\n",
        "        combined_test_sentence = ' '.join(first_test_df['Label'].tolist())\n",
        "\n",
        "        # Create Combined_df with the combined sentence\n",
        "        combined_test_df = pd.DataFrame({'Transcript': [combined_test_sentence]})\n",
        "\n",
        "        # Find the highest occurring strings in second_df\n",
        "        highest_occurrences = second_test_df['Label'].mode()\n",
        "\n",
        "        if not highest_occurrences.empty:\n",
        "            # Choose a random element from the list of highest occurrences\n",
        "            phoneme_likelihood_test = random.choice(highest_occurrences.tolist())\n",
        "        else:\n",
        "            phoneme_likelihood_test = None  # Handle the case where there are no labels\n",
        "        # Create Transcript DataFrame with the highest occurring string\n",
        "        transcript_test_df = pd.DataFrame({'phoneme_likelihood': [phoneme_likelihood_test]})\n",
        "\n",
        "        # Create Phones DataFrame with the list of phones\n",
        "        phones_test_df = pd.DataFrame({'Phones': [second_test_df['Label'].tolist()]})\n",
        "\n",
        "        # Extract the MFCC mel spectrogram\n",
        "        audio_file_path = os.path.join(wav_test_files_directory, filename[:-9] + '.wav')\n",
        "        mfcc_spectogram = extract_mfcc_spectrogram(audio_file_path)\n",
        "\n",
        "        # Extract the Mel spectrograms\n",
        "        mel_spectrograms = extract_mel_spectrogram(audio_file_path)\n",
        "\n",
        "        # Create MFCC Spectrogram DataFrame with the MFCC mel spectrogram\n",
        "        mfcc_spectrogram_df = pd.DataFrame({'MFCC_Spectrogram': [mfcc_spectrogram]})\n",
        "\n",
        "        # Create Mel Spectrogram DataFrame with the mel spectrogram\n",
        "        mel_spectrograms_df = pd.DataFrame({'Mel_Spectrograms': [mel_spectrograms]})\n",
        "\n",
        "        # Concatenate the DataFrames and append to the final_df\n",
        "        result_test_df = pd.concat([combined_test_df, transcript_test_df, phones_test_df, mfcc_spectrogram_df, mel_spectrograms_df], axis=1)\n",
        "        final_test_df = pd.concat([final_test_df, result_test_df], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnAmf2uJjYZY",
        "outputId": "7a467ae4-e3ab-48b1-8c36-c80d8e813a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "181/181 [==============================] - 60s 289ms/step - loss: 3.9080 - accuracy: 0.1941 - val_loss: 2.6482 - val_accuracy: 0.2701\n",
            "Epoch 2/10\n",
            "181/181 [==============================] - 51s 279ms/step - loss: 2.4216 - accuracy: 0.4219 - val_loss: 2.1544 - val_accuracy: 0.4439\n",
            "Epoch 3/10\n",
            "181/181 [==============================] - 50s 275ms/step - loss: 1.9387 - accuracy: 0.5108 - val_loss: 2.0207 - val_accuracy: 0.4898\n",
            "Epoch 4/10\n",
            "181/181 [==============================] - 47s 263ms/step - loss: 1.6832 - accuracy: 0.5658 - val_loss: 2.0846 - val_accuracy: 0.4870\n",
            "Epoch 5/10\n",
            "181/181 [==============================] - 48s 262ms/step - loss: 1.4386 - accuracy: 0.6248 - val_loss: 2.1462 - val_accuracy: 0.4718\n",
            "Epoch 6/10\n",
            "181/181 [==============================] - 47s 258ms/step - loss: 1.2438 - accuracy: 0.6796 - val_loss: 2.3783 - val_accuracy: 0.4463\n",
            "Epoch 7/10\n",
            "181/181 [==============================] - 47s 261ms/step - loss: 1.0780 - accuracy: 0.7215 - val_loss: 2.5445 - val_accuracy: 0.4532\n",
            "Epoch 8/10\n",
            "181/181 [==============================] - 45s 249ms/step - loss: 0.9684 - accuracy: 0.7462 - val_loss: 2.6725 - val_accuracy: 0.4532\n",
            "Epoch 9/10\n",
            "181/181 [==============================] - ETA: 0s - loss: 0.8477 - accuracy: 0.7748Epoch 10/10\n",
            "181/181 [==============================] - 46s 256ms/step - loss: 0.7716 - accuracy: 0.7944 - val_loss: 2.9898 - val_accuracy: 0.4380\n",
            "362/362 [==============================] - 15s 42ms/step - loss: 0.4824 - accuracy: 0.8730\n",
            "91/91 [==============================] - 3s 33ms/step - loss: 2.9898 - accuracy: 0.4380\n",
            "Training Accuracy: 0.8730350732803345\n",
            "Validation Accuracy: 0.4379965364933014\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization, GlobalMaxPooling1D, SpatialDropout1D, Flatten, Input, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import LSTM, MaxPooling1D\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "X_transcript = final_df['Transcript']\n",
        "X_MFCC_spectrograms = final_df['MFCC_Spectrogram']\n",
        "y = final_df['phoneme_likelihood']\n",
        "\n",
        "#  the data into training and test sets\n",
        "X_train_transcript, X_test_transcript, X_train_MFCC_spectrograms, X_test_MFCC_spectrograms, y_train, y_test = train_test_split(\n",
        "    X_transcript, X_MFCC_spectrograms, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the transcript training data\n",
        "tokenizer.fit_on_texts(X_train_transcript)\n",
        "\n",
        "# Convert transcript text to sequences\n",
        "X_train_transcript_seq = tokenizer.texts_to_sequences(X_train_transcript)\n",
        "X_test_transcript_seq = tokenizer.texts_to_sequences(X_test_transcript)\n",
        "\n",
        "# Pad transcript sequences to have the same length\n",
        "max_length_transcript = max(max(len(seq) for seq in X_train_transcript_seq), max(len(seq) for seq in X_test_transcript_seq))\n",
        "X_train_transcript_padded = pad_sequences(X_train_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "X_test_transcript_padded = pad_sequences(X_test_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "\n",
        "# Find the maximum sequence length for transcript\n",
        "max_length_transcript = max(len(seq) for seq in X_train_transcript_seq + X_test_transcript_seq)\n",
        "\n",
        "# Find the maximum number of features for spectrograms\n",
        "max_features_spectrogram = max(arr.shape[1] for arr in X_train_MFCC_spectrograms + X_test_MFCC_spectrograms)\n",
        "\n",
        "# Pad or truncate the transcript sequences to have the same length\n",
        "X_train_transcript_padded = pad_sequences(X_train_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "X_test_transcript_padded = pad_sequences(X_test_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "\n",
        "# Pad or truncate the spectrogram arrays to have the same number of features\n",
        "X_train_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :max_features_spectrogram], ((0, 0), (0, max_features_spectrogram - arr.shape[1])), mode='constant') for arr in X_train_MFCC_spectrograms])\n",
        "X_test_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :max_features_spectrogram], ((0, 0), (0, max_features_spectrogram - arr.shape[1])), mode='constant') for arr in X_test_MFCC_spectrograms])\n",
        "\n",
        "\n",
        "# Encode the response variable\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y)  # Fit the label encoder on all labels in y\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Find the maximum number of features for spectrograms\n",
        "max_features_spectrogram = max(max(arr.shape[1] for arr in X_train_MFCC_spectrograms), max(arr.shape[1] for arr in X_test_MFCC_spectrograms))\n",
        "\n",
        "# Pad or truncate the spectrogram arrays to have the same number of features\n",
        "X_train_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :777], ((0, 0), (0, 777 - arr.shape[1])), mode='constant') if arr.shape[1] < 777 else arr[:, :777] for arr in X_train_MFCC_spectrograms])\n",
        "X_test_MFCC_spectrograms_padded = np.array([np.pad(arr[:, :777], ((0, 0), (0, 777 - arr.shape[1])), mode='constant') if arr.shape[1] < 777 else arr[:, :777] for arr in X_test_MFCC_spectrograms])\n",
        "\n",
        "# Define the model\n",
        "input_transcript = Input(shape=(max_length_transcript,))\n",
        "embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(input_transcript)\n",
        "conv1d_transcript = Conv1D(filters=64, kernel_size=3, activation='relu')(embedding)\n",
        "conv1d_transcript = BatchNormalization()(conv1d_transcript)\n",
        "conv1d_transcript = GlobalMaxPooling1D()(conv1d_transcript)\n",
        "conv1d_transcript = Dropout(0.5)(conv1d_transcript)\n",
        "dense = Dense(units=128, activation='relu', kernel_regularizer=l2(0.0001))(conv1d_transcript)\n",
        "dense = Dropout(0.5)(dense)\n",
        "\n",
        "# Add LSTM layer to the transcript part\n",
        "lstm_transcript = LSTM(64)(embedding)\n",
        "lstm_transcript = Dropout(0.5)(lstm_transcript)\n",
        "\n",
        "# Merge the LSTM and Conv1D features\n",
        "merged_features = Concatenate()([dense, lstm_transcript])\n",
        "input_spectrogram = Input(shape=(39, max_features_spectrogram))\n",
        "conv1d = Conv1D(filters=128, kernel_size=5, activation='relu')(input_spectrogram)\n",
        "conv1d = MaxPooling1D(pool_size=2)(conv1d)\n",
        "conv1d = BatchNormalization()(conv1d)\n",
        "conv1d = Dropout(0.5)(conv1d)\n",
        "\n",
        "# Add another Conv1D layer\n",
        "conv1d = Conv1D(filters=128, kernel_size=3, activation='relu')(conv1d)\n",
        "conv1d = MaxPooling1D(pool_size=2)(conv1d)\n",
        "conv1d = BatchNormalization()(conv1d)\n",
        "conv1d = Dropout(0.5)(conv1d)\n",
        "\n",
        "# Flatten the Conv1D output\n",
        "conv1d = Flatten()(conv1d)\n",
        "\n",
        "# Merge the transcript and spectrogram features\n",
        "merged_features = Concatenate()([conv1d, merged_features])\n",
        "\n",
        "dense = Dense(units=256, activation='relu')(merged_features)\n",
        "dense = BatchNormalization()(dense)\n",
        "dense = Dropout(0.5)(dense)\n",
        "\n",
        "output = Dense(units=len(label_encoder.classes_), activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=[input_transcript, input_spectrogram], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "#early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_train_transcript_padded, X_train_MFCC_spectrograms_padded], y_train_encoded,\n",
        "          epochs=10, batch_size=64, validation_data=([X_test_transcript_padded, X_test_MFCC_spectrograms_padded], y_test_encoded))\n",
        "\n",
        "# Calculate training and validation accuracy\n",
        "_, train_accuracy = model.evaluate([X_train_transcript_padded, X_train_MFCC_spectrograms_padded], y_train_encoded)\n",
        "_, test_accuracy = model.evaluate([X_test_transcript_padded, X_test_MFCC_spectrograms_padded], y_test_encoded)\n",
        "\n",
        "print('Training Accuracy:', train_accuracy)\n",
        "print('Validation Accuracy:', test_accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9XDMH5hQYVw",
        "outputId": "5547b12e-5fff-44a1-f6ee-36cd9313b526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102/102 [==============================] - 4s 32ms/step\n",
            "['D' 'R' 'B' ... 'AH0' 'R' 'AH0']\n",
            "Accuracy on New Data: 0.31109744850906856\n"
          ]
        }
      ],
      "source": [
        "# Extract the input features from the final test data\n",
        "X_new_mfcc_spectrograms = final_test_df['MFCC_Spectrogram']\n",
        "X_new_transcripts = final_test_df['Transcript']\n",
        "\n",
        "# Convert text to sequences using the tokenizer fitted on the training data\n",
        "X_new_transcript_seq = tokenizer.texts_to_sequences(X_new_transcripts)\n",
        "X_new_transcript_padded = pad_sequences(X_new_transcript_seq, maxlen=max_length_transcript, padding='post')\n",
        "\n",
        "#print(X_new_spectrograms.shape, X_new_spectrograms.dtype)\n",
        "# Make predictions on the new data\n",
        "# Find the maximum number of features for spectrograms\n",
        "\n",
        "\n",
        "# Pad or truncate the spectrogram arrays to have the same number of features\n",
        "X_new_mfcc_spectrograms_padded = np.array([np.pad(arr[:, :777], ((0, 0), (0, 777 - arr.shape[1])), mode='constant') if arr.shape[1] < 777 else arr[:, :777] for arr in X_new_mfcc_spectrograms])\n",
        "predictions = model.predict([X_new_transcript_padded, X_new_mfcc_spectrograms_padded])\n",
        "\n",
        "# Decode the predicted labels\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "print(predicted_labels)\n",
        "\n",
        "# Calculate the accuracy on the new data\n",
        "accuracy = np.mean(predicted_labels == final_test_df['phoneme_likelihood'])\n",
        "print(\"Accuracy on New Data:\", accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}