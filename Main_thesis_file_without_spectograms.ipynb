{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiteshJindal/Thesis_Audio_Uncertainity/blob/main/Main_thesis_file_without_spectograms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yZ3pVqifdpy"
      },
      "outputs": [],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49eSWpBofV_N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pydub.utils import mediainfo\n",
        "from pydub import AudioSegment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZmSPyaNfmuL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# Set the audio directory\n",
        "#audio_dir = '/gdrive/MyDrive/Audio_Dataset/textgrid/common_voice_en_33170883.TextGrid'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LpFHg3yelLo1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Create an empty DataFrame to store the results\n",
        "final_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'Phones'])\n",
        "\n",
        "directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/Textgrid_done_big_wav_transcription/'\n",
        "\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "    start_time, end_time, label = None, None, None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('xmin'):\n",
        "            start_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('xmax'):\n",
        "            end_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('text'):\n",
        "            label = line.split('=')[1].strip().strip('\"')\n",
        "            if start_time is not None and end_time is not None and label is not None:\n",
        "                data.append((start_time, end_time, label))\n",
        "                start_time, end_time, label = None, None, None\n",
        "\n",
        "    return data\n",
        "\n",
        "def textgrid_to_dataframe(file_path):\n",
        "    data = parse_textgrid(file_path)\n",
        "    df = pd.DataFrame(data, columns=['Start Time', 'End Time', 'Label'])\n",
        "    return df\n",
        "\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.TextGrid'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Process the file and obtain the necessary dataframes\n",
        "        df = textgrid_to_dataframe(file_path)\n",
        "\n",
        "        # Get the indices of the matched rows\n",
        "        indices = df.index[(df['Start Time'] == df['Start Time'].iloc[0]) & (df['End Time'] == df['End Time'].iloc[0])]\n",
        "\n",
        "        # Split the DataFrame based on indices\n",
        "        first_df = df.loc[:indices[-1]]\n",
        "        second_df = df.loc[indices[-1]+1:]\n",
        "\n",
        "        # Remove rows with blank or null labels from first_df\n",
        "        first_df = first_df[first_df['Label'].notnull() & (first_df['Label'] != \"\")]\n",
        "\n",
        "        # Remove rows with blank or null labels from second_df\n",
        "        second_df = second_df[second_df['Label'].notnull() & (second_df['Label'] != \"\")]\n",
        "\n",
        "        # Combine labels from first_df into a single sentence\n",
        "        combined_sentence = ' '.join(first_df['Label'].tolist())\n",
        "\n",
        "        # Create Combined_df with the combined sentence\n",
        "        combined_df = pd.DataFrame({'Transcript': [combined_sentence]})\n",
        "\n",
        "        # Find the highest occurring string in second_df\n",
        "        phoneme_likelihood = second_df['Label'].mode().iloc[0]\n",
        "\n",
        "        # Create Transcript DataFrame with the highest occurring string\n",
        "        transcript_df = pd.DataFrame({'phoneme_likelihood': [phoneme_likelihood]})\n",
        "\n",
        "        # Combine labels from second_df into a list\n",
        "        phones_list = second_df['Label'].tolist()\n",
        "\n",
        "        # Create Phones DataFrame with the list of phones\n",
        "        phones_df = pd.DataFrame({'Phones': [phones_list]})\n",
        "\n",
        "        # Concatenate the DataFrames and append to the final_df\n",
        "        result_df = pd.concat([combined_df, transcript_df, phones_df], axis=1)\n",
        "        final_df = pd.concat([final_df, result_df], ignore_index=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gA8mEr-5qBWj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Create an empty DataFrame to store the results\n",
        "final_test_df = pd.DataFrame(columns=['Transcript', 'phoneme_likelihood', 'Phones'])\n",
        "\n",
        "directory = '/gdrive/MyDrive/Input_large_final/Input_large/senddrive/wav_textgrid_big_test/'\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "    start_time, end_time, label = None, None, None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('xmin'):\n",
        "            start_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('xmax'):\n",
        "            end_time = float(line.split('=')[1].strip())\n",
        "        elif line.startswith('text'):\n",
        "            label = line.split('=')[1].strip().strip('\"')\n",
        "            if start_time is not None and end_time is not None and label is not None:\n",
        "                data.append((start_time, end_time, label))\n",
        "                start_time, end_time, label = None, None, None\n",
        "\n",
        "    return data\n",
        "\n",
        "def textgrid_to_dataframe(file_path):\n",
        "    data = parse_textgrid(file_path)\n",
        "    df = pd.DataFrame(data, columns=['Start Time', 'End Time', 'Label'])\n",
        "    return df\n",
        "\n",
        "# Iterate over the files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.TextGrid'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Process the file and obtain the necessary dataframes\n",
        "        df = textgrid_to_dataframe(file_path)\n",
        "\n",
        "        # Get the indices of the matched rows\n",
        "        indices = df.index[(df['Start Time'] == df['Start Time'].iloc[0]) & (df['End Time'] == df['End Time'].iloc[0])]\n",
        "\n",
        "        # Split the DataFrame based on indices\n",
        "        first_test_df = df.loc[:indices[-1]]\n",
        "        second_test_df = df.loc[indices[-1]+1:]\n",
        "\n",
        "        # Remove rows with blank or null labels from first_df\n",
        "        first_test_df = first_test_df[first_test_df['Label'].notnull() & (first_test_df['Label'] != \"\")]\n",
        "\n",
        "        # Remove rows with blank or null labels from second_df\n",
        "        second_test_df = second_test_df[second_test_df['Label'].notnull() & (second_test_df['Label'] != \"\")]\n",
        "\n",
        "        # Combine labels from first_df into a single sentence\n",
        "        combined_sentence_test = ' '.join(first_test_df['Label'].tolist())\n",
        "\n",
        "        # Create Combined_df with the combined sentence\n",
        "        combined_test_df = pd.DataFrame({'Transcript': [combined_sentence_test]})\n",
        "\n",
        "        # Find the highest occurring strings in second_df\n",
        "        highest_occurrences = second_test_df['Label'].mode()\n",
        "\n",
        "        if not highest_occurrences.empty:\n",
        "            # Choose a random element from the list of highest occurrences\n",
        "            phoneme_likelihood_test = random.choice(highest_occurrences.tolist())\n",
        "        else:\n",
        "            phoneme_likelihood_test = None  # Handle the case where there are no labels\n",
        "\n",
        "        # Create Transcript DataFrame with the highest occurring string\n",
        "        transcript_test_df = pd.DataFrame({'phoneme_likelihood': [phoneme_likelihood_test]})\n",
        "\n",
        "        # Create Phones DataFrame with the list of phones\n",
        "        phones_test_df = pd.DataFrame({'Phones': [second_test_df['Label'].tolist()]})\n",
        "\n",
        "        # Concatenate the DataFrames and append to the final_df\n",
        "        result_test_df = pd.concat([combined_test_df, transcript_test_df, phones_test_df], axis=1)\n",
        "        final_test_df = pd.concat([final_test_df, result_test_df], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7ykV3cF2qnX1"
      },
      "outputs": [],
      "source": [
        "print(final_test_df.shape)\n",
        "print(final_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cPJjSv5SeudQ"
      },
      "outputs": [],
      "source": [
        "final_df.head(150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to6TClXOpoiD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Flatten, BatchNormalization, MaxPooling1D, SimpleRNN\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam, Nadam\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense, LSTM, Reshape\n",
        "import tensorflow as tf\n",
        "\n",
        "# Example with L2 regularization for dense layers\n",
        "\n",
        "# Extract the input features and response variable\n",
        "X = final_df['Transcript']\n",
        "y = final_df['phoneme_likelihood']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the training data\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_length = max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_test_seq))\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Encode the response variable\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y)  # Fit the label encoder on all labels in y\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define the improved model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_length))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "# Add another Conv1D layer\n",
        "model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Define the model checkpoint callback to save the best model during training\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_padded, y_train_encoded, epochs=20, batch_size=128, validation_data=(X_test_padded, y_test_encoded))\n",
        "\n",
        "\n",
        "# Calculate training and validation accuracy\n",
        "train_accuracy = model.evaluate(X_train_padded, y_train_encoded)[1]\n",
        "val_accuracy = model.evaluate(X_test_padded, y_test_encoded)[1]\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Validation Accuracy:\", val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc_fWCr5pfwx"
      },
      "outputs": [],
      "source": [
        "# Assume you have a new DataFrame called 'new_data' with the same column names as 'final_df'\n",
        "\n",
        "# Extract the input features from the new data\n",
        "X_new = final_test_df['Transcript']\n",
        "\n",
        "# Convert text to sequences using the tokenizer fitted on the training data\n",
        "X_new_seq = tokenizer.texts_to_sequences(X_new)\n",
        "\n",
        "# Pad sequences to have the same length as the training data\n",
        "X_new_padded = pad_sequences(X_new_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Make predictions on the new data\n",
        "predictions = model.predict(X_new_padded)\n",
        "\n",
        "# Decode the predicted labels\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "print(predicted_labels)\n",
        "print(final_test_df['phoneme_likelihood'])\n",
        "# Calculate the accuracy on the new data\n",
        "accuracy = np.mean(predicted_labels == final_test_df['phoneme_likelihood'])\n",
        "\n",
        "print(\"Accuracy on New Data:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjdyW920rTCZ"
      },
      "outputs": [],
      "source": [
        "print(np.sum(predicted_labels == final_test_df['phoneme_likelihood']))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}